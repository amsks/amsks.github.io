<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Bayesian Optimization | Aditya Mohan</title> <meta name="author" content="Aditya Mohan"> <meta name="description" content="Personal Website of Aditya Mohan "> <meta name="keywords" content="Reinforcement Learning, Meta-Learning, AutoML, AutoRL"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%A8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://amsks.github.io/blog/2023/bayesian-optimization/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"><span class="font-weight-bold">Aditya </span>Mohan</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Hey there!</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bayesian Optimization</h1> <p class="post-meta">July 8, 2023</p> <p class="post-tags"> <a href="//blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="//blog/category/automl"> <i class="fas fa-tag fa-sm"></i> automl</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The general optimization problem can be stated as the task of finding the minimal point of some objective function by adhering to certain constraints. More formally, we can write it as</p> \[\min_x f(x) \,\,\,\, s.t \,\,\,\, g(x) \leq 0 \,\,\,, \,\,\, h(x) = 0\] <p>We usually assume that our functions \(f, g, h\) are differentiable, and depending on how we calculate the first and second-order gradients (The Jacobians and Hessians) of our function, we designate the different kinds of methods used to solve this problem. Thus, in a first-order optimization problem, we can evaluate our objective function as well as the Jacobian, while in a second-order problem we can even evaluate the Hessian. In other cases, we impose some other constraints on either the form of our objective function or do some tricks to approximate the gradients, like approximating the Hessians in Quasi-Newton optimization. However, these do not cover cases where \(f(x)\) is a black box. Since we cannot assume that we fully know this function our task can be re-formulated as finding this optimal point \(x\) while discovering this function \(f\). This can be written in the same form, just without the constraints</p> \[\min _x f(x)\] <h2 id="kwik">KWIK</h2> <p>To find the optimal \(x\) for an unknown \(f\) we need to explicitly reason about what we know about \(f\). This is the <strong>Knows What It Knows</strong> framework. I will present an example from the paper that helps understand the need for this explicit reasoning about our function. Consider the task of navigating the following graph:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/AutoML/KWIK-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/AutoML/KWIK-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/AutoML/KWIK-1400.webp"></source> <img src="/assets/img/AutoML/KWIK.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Each edge in the graph is associated with a binary cost and let’s assume that the agent does not know about these costs beforehand, but knows about the topology of the graph. Each time an agent moves from one node to another, it observes and accumulates the cost. An episode is going from the source on the left to the sink on the right. Hence, the learning task is to figure out the optimal path in a few episodes. The simplest solution for the agent is to assume that the costs of edges are uniform and thus, take the shortest path through the middle, which gives it a total cost of 13. We could then use a standard regression algorithm to fit a weight vector to this dataset and estimate the cost of the other paths, simply based on the nodes observed so far, which gives us 14 for the top, 13 for the middle, and 14 for the bottom paths. Hence, the agent would choose to take the middle path, even though it is suboptimal as compared to the top one.</p> <p>Now, let’s consider an agent that does not just fit a weight vector but reasons about whether it can obtain the cost of edges with the available data. Assuming the agent completed the first episode through the middle path and accumulated a reward of 13, the question it needs to answer is which path to go for next. In the bottom path cost of the penultimate node is 2, which can be figured out from the costs of nodes already visited</p> \[3 - 1 = 2\] <p>This gives us more certainty than the uniform assumption that we started with. However, this kind of dependence does not really exist for the upper node since the linear combination does not work on the nodes already visited. If we incorporate a way for our agent to say that it is not sure about the answer to the cost of the upper nodes, we can essentially incentivize it to explore the upper node in the next round, allowing our agent to visit this node and discover the optimal solution. This is similar to how we discuss the exploration-exploitation dilemma in Reinforcement Learning.</p> <h2 id="mdp-framework">MDP framework</h2> <p>Motivated from the previous section and based on the treatment done <a href="https://www.user.tu-berlin.de/mtoussai//teaching/Lecture-Maths.pdf" rel="external nofollow noopener" target="_blank">here</a>, we can model our solver as an agent and the function as the environment. Our agent can sample the value of the function in a range of possible values and in a limited budget of samples, it needs to find the optimal \(x\). The observation that comes after sampling from the environment is the noisy estimate of \(f\), which can call \(y\). Thus, we can write our function as the expectation over these outputs</p> \[f( x) = \mathbb{E}\big [ y |f(x) \big ]\] <p>We can cast this as a Markov Decision Process where the state is defined by the data the agent has collected so far. Let’s call this data \(S\). Thus, at each iteration \(t\), our agent exists in a state \(S_t\) and needs to make a decision on where to sample the next \(x_t\). Once it collects this sample, it adds this to its existing knowledge</p> \[S_{t+1} = S_t \cup \{x_t, f_t \}\] <p>We can create a policy \(\pi\) that our agent follows to take an action from a particular state</p> \[\pi : S_t \rightarrow x_t\] <table> <tbody> <tr> <td>Hence, the agent operates with a prior over our function \(P(f)\) , and based on this prior it calculates a deterministic posterior $$P_\pi (S</td> <td>x_t, f)$$ by multiplying it with the expectation over the outputs.</td> </tr> </tbody> </table> \[\pi ^* \in \text{argmin}_\pi \int P(f) P( S|\pi , f) \mathbb{E}[y|f]\] <p>Since the agent does not know \(f\) apriori, it needs to calculate a posterior belief over this function based on the accumulated data</p> \[P(f|S) = \frac{P(S|f) P(f)}{P(S)}\] <table> <tbody> <tr> <td>With the incorporation of this belief, we can define an MDP over the beliefs with stochastic transitions. The states in this MDP are the posterior belief $$P(f</td> <td>S)$$ . Thus, the agent needs to simulate the transitions in this MDP and it can theoretically solve the optimal problem through something like Dynamic programming. However, this is difficult to compute.</td> </tr> </tbody> </table> <h2 id="bayesian-methods">Bayesian Methods</h2> <table> <tbody> <tr> <td>This is where Bayesian methods come into the picture. They formulate this belief $$P(f</td> <td>S)$$ as a Bayesian representation and compute this using a gaussian process at every step. After this, they use a heuristic to choose the next decision. The Gaussian process used to compute this belief is called <strong>surrogate function</strong> and the heuristic used is called an <strong>Acquisition Function.</strong> We can write the process as follows:</td> </tr> </tbody> </table> <ol> <li>Compute the posterior belief using a surrogate Gaussian process to form an estimate of the mean \(\mu(x)\) and variance around this estimate \(\sigma^2(x)\) to describe the uncertainty</li> <li>Compute an acquisition function \(\alpha_t(x)\) that is proportional to how beneficial it is to sample the next point from the range of values</li> <li> <p>Find the maximal point of this acquisition function and sample at that next location</p> \[x_t = \argmax_x \alpha_t(x)\] </li> </ol> <p>This process is repeated a fixed number of iterations called the <strong>optimization budget</strong> to converge to a decently good point. Three poplar acquisition functions are</p> <ul> <li> <p><strong>Probability of Improvement (MPI) →</strong> The value of the acquisition function is proportional to the probability of improvement at each point. We can characterize this as the upper-tail CDF of the surrogate posterior</p> \[\alpha_t( x) = \int_{-\infty}^{y_{opt}}\mathcal{N} \big (y|\mu(x), \sigma (x) \big ) dy\] </li> <li> <p><strong>Expected Improvement (EI)</strong> → The value is not just proportional to the probability, but also to the magnitude of possible improvement from the point.</p> \[\alpha_t(x) = \int_{-\infty}^{y_{opt}}\mathcal{N} \big (y|\mu(x), \sigma (x) \big ) \big [ y_{opt} - y\big ] dy\] </li> <li> <p><strong>Upper Confidence Bound (UCB)</strong> → We control the exploration through the variance and control parameter and exploit the maximum values</p> \[\alpha_t(x) = -\mu(x) + \beta\sigma(x)\] </li> </ul> <p>The evaluation of this maximization of the acquisition function is another non-linear optimization problem. However, the advantage is that these functions are analytic and so, we can solve for jacobians and Hessians of these, ensuring convergence at least on a local level. To make this process converge globally, we need to optimize from multiple start points from the domain and hope that after all these random starts the maximum found by the algorithm is indeed the global one.</p> <h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2> <p>One of the places where Global Bayesian Optimization can show good results is the optimization of hyperparameters for Neural Networks. So, let’s implement this approach to tune the learning rate of an Image Classifier! I will use the KMNIST dataset and a small ResNet-9 Model with a Stochastic Gradient Descent optimizer. Our plan of attack is as follows:</p> <ol> <li>Create a training pipeline for our Neural Network with the Dataset and customizable learning rate</li> <li>Cast the training and inference into a an objective function, which can serve as ou blackbox</li> <li>Map the inference to an evaluation metric that can be used in the optimization procedure</li> <li>Use this function in a global bayesian optimization procedure.</li> </ol> <h3 id="creating-the-training-pipeline-and-objective-function">Creating the training pipeline and Objective Function</h3> <p>I have used PyTorch and the lightning module to create a boilerplate that can be used to train our network. Since KMNIST and ResNet architectures are already available in PyTorch, all we need to do is customize the ResNet architecture for MNIST, which I have done as follows</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_resnet9_model</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sh">'''</span><span class="s">
        Function to customize the RESNET to 9 layers and 10 classes

        Returns
        --------
        torch.module
            Pytorch Module of the Model
    </span><span class="sh">'''</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># 
</span><span class="k">class</span> <span class="nc">ResNet9</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
            Pytorch Lightning Module for training the RESNET with SGD optimizer

            Parameters
            -----------
            learning_rate: float 
                Learning rate to be used for training every time since it is an 
                optimization parameter
        </span><span class="sh">'''</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nf">create_resnet9_model</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

    <span class="nd">@auto_move_data</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_no</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="nf">self</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div> <p>Once this is done, our next step is to use the training pipeline and cast it into an objective function. For this, we need to evaluate our model somehow. I have used the balanced accuracy as an evaluation metric, but any other metric can also be used (like the AUC-ROC score)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span>  <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                <span class="n">gpu_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                <span class="n">iteration</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                <span class="n">model_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">./outputs/models/</span><span class="sh">'</span><span class="p">,</span> 
                <span class="n">train_dl</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">test_dl</span> <span class="o">=</span> <span class="bp">None</span> 
            <span class="p">):</span>

    <span class="sh">'''</span><span class="s">
        The objective function for the optimization procedure 

        Parameters
        -----------
        lr: float 
            learning Rate 
        epochs: int 
            Epochs for training
        gpu_count: int 
            Number of GPUs to be used (0 for only CPUs)
        iteration: int 
            Current iteration
        model_dir: str
            directory to save model checkpoints 
        train_dl: Torch Dataloader 
            Dataloader for training
        test_dl: Torch Dataloader 
            Dataloader for inference

        Returns
        ---------
        float
            balanced Accuracy of the model after inference

    </span><span class="sh">'''</span>

    <span class="n">save</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="sh">"</span><span class="s">current_model.pt</span><span class="sh">"</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet9</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="nc">Trainer</span><span class="p">(</span>
        <span class="n">gpus</span><span class="o">=</span><span class="n">gpu_count</span><span class="p">,</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">progress_bar_refresh_rate</span><span class="o">=</span><span class="mi">20</span>
    <span class="p">)</span>

    <span class="n">trainer</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">)</span>
    <span class="n">trainer</span><span class="p">.</span><span class="nf">save_checkpoint</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

    <span class="n">inference_model</span> <span class="o">=</span> <span class="n">ResNet9</span><span class="p">.</span><span class="nf">load_from_checkpoint</span><span class="p">(</span>
        <span class="n">checkpoint</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">true_y</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">prob_y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">test_dl</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">test_dl</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">true_y</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
				<span class="n">model</span><span class="p">.</span><span class="nf">freeze</span><span class="p">()</span>
		    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="nf">inference_model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		    <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pred_y</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">predicted_class</span><span class="p">.</span><span class="nf">cpu</span><span class="p">())</span>
        <span class="n">prob_y</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">probabilities</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">save</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
        <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">balanced_accuracy_score</span><span class="p">(</span><span class="n">true_y</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">))</span>
</code></pre></div></div> <h3 id="implementing-bayesian-optimization">Implementing Bayesian Optimization</h3> <p>As mentioned in the previous sections, we first need a Gaussian Process as a surrogate model. We can either write it from scratch or just use some open-sourced library to do this. Here, I have used sci-kit learn to create a regressor</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create the Model
</span>    <span class="n">m52</span> <span class="o">=</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">gaussian_process</span><span class="p">.</span><span class="nf">kernelsConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="nc">Matern</span><span class="p">(</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> 
                                        <span class="n">nu</span><span class="o">=</span><span class="mf">1.5</span>
                                    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">gaussian_process</span><span class="p">.</span><span class="nc">GaussianProcessRegressor</span><span class="p">(</span>
                                        <span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> 
                                        <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> 
                                        <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">100</span>
                                    <span class="p">)</span>
</code></pre></div></div> <p>Once th Gaussian process is established, we now need to write the acquisition function. I have used the Expected Improvements acquisition function. The core idea can be re-written as proposed by Mockus</p> \[EI(x) = \begin{cases} \big( \mu_t(x) - y_{max} - \epsilon \big ) \Phi(Z) + \sigma_t (x) \phi(Z) &amp;\sigma_t(x) &gt; 0 \\ 0 &amp; \sigma_t(x) &gt; 0 \end{cases}\] <p>Where</p> \[Z = \frac{\mu_t(x) - y_{max} - \epsilon}{\sigma_t(x) }\] <p>and \(\Phi\) and \(\phi\) are the PDF and CDF functions. This formulation is an analytical expression that achieves the same result as our earlier formulation and we have added \(\epsilon\) as an exploration parameter. This can be implemented as follows</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_acquisition</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
            Acquisition function using hte Expected Improvement method

            Parameters
            -----------
            X : N x 1 
                Array of parameter points observed so far

            X_samples : N x 1
                Array of Sampled points between the bounds

            Returns
            --------
            float
                Expected improvement

        </span><span class="sh">'''</span>

        <span class="c1"># calculate the max of surrogate values from history
</span>        <span class="n">mu_x_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">surrogate</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">max_x_</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">mu_x_</span><span class="p">)</span>

        <span class="c1"># Get the mean and deviation of the samples 
</span>        <span class="n">mu_sample_</span><span class="p">,</span> <span class="n">std_sample_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">surrogate</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">mu_sample_</span> <span class="o">=</span> <span class="n">mu_sample_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Get the improvement
</span>        <span class="k">with</span> <span class="n">np</span><span class="p">.</span><span class="nf">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="sh">'</span><span class="s">warn</span><span class="sh">'</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu_sample_</span> <span class="o">-</span> <span class="n">max_x_</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">/</span> <span class="n">std_sample_</span>
            <span class="n">EI_</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu_sample_</span> <span class="o">-</span> <span class="n">max_x_</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> \
                <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">std_sample_</span> <span class="o">*</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">EI_</span><span class="p">[</span><span class="n">std_sample_</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">EI_</span>
</code></pre></div></div> <p>the <code class="language-plaintext highlighter-rouge">self.surrogate()</code> function is just predicting using the Gaussian process earlier written. Once we have our expected improvements, we need to optimize our acquisition by maximizing over these expected improvements</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">optimize_acq</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
            Optimization of the Acquisition function using a maximization check of the outputs

            Parameters
            -----------
            X : N x 1 
                Array of parameter points

            Returns
            --------
            float
                Next location of the sampling point based on the Maximization

        </span><span class="sh">'''</span>
            
        <span class="c1"># Calculate Acquisition value for each sample
</span>        <span class="n">EI_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">X_samples_</span><span class="p">)</span>

        <span class="c1"># Get the index of the largest Score
</span>        <span class="n">max_index_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">EI_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">X_samples_</span><span class="p">[</span><span class="n">max_index_</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <h3 id="putting-it-all-together">Putting it all together</h3> <p>Now that we have our optimization routines, we just need to combine them with our objective function into a loop and we are done. In my code, I have implemented the optimization as a class and I pass the paramters to this class. So, the main loop looks as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">budget</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="nc">KMNIST</span><span class="p">(</span><span class="sh">"</span><span class="s">kmnist</span><span class="sh">"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="nc">KMNIST</span><span class="p">(</span><span class="sh">"</span><span class="s">kmnist</span><span class="sh">"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>

<span class="n">train_dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span> <span class="p">)</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># sample the domain
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">init_samples</span><span class="p">)])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">objective</span><span class="p">(</span><span class="n">lr</span> <span class="o">=</span><span class="n">x</span><span class="p">,</span> 
                        <span class="n">epochs</span><span class="o">=</span><span class="n">init_epochs</span><span class="p">,</span> 
                        <span class="n">gpu_count</span><span class="o">=</span><span class="n">gpu_count</span><span class="p">,</span>
                        <span class="n">train_dl</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span>
                        <span class="n">test_dl</span><span class="o">=</span><span class="n">test_dl</span>
                        <span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">budget</span><span class="p">):</span>
		<span class="c1"># fit the model
</span>		<span class="n">B</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
		
		<span class="c1"># Select the next point to sample
</span>		<span class="n">X_next</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">optimize_acq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
		
		<span class="c1"># Sample the point from Objective
</span>		<span class="n">Y_next</span> <span class="o">=</span> <span class="nf">objective</span><span class="p">(</span> <span class="n">lr</span><span class="o">=</span><span class="n">X_next</span><span class="p">,</span> 
		                    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
		                    <span class="n">gpu_count</span><span class="o">=</span><span class="n">gpu_count</span><span class="p">,</span>
		                    <span class="n">model_dir</span><span class="o">=</span> <span class="n">output_dir</span><span class="o">+</span><span class="sh">"</span><span class="s">/models/</span><span class="sh">"</span><span class="p">,</span> 
		                    <span class="n">iteration</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
		                    <span class="n">train_dl</span><span class="o">=</span> <span class="n">train_dl</span><span class="p">,</span>
		                    <span class="n">test_dl</span> <span class="o">=</span> <span class="n">test_dl</span>
		                    <span class="p">)</span>
		
		<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">LR = </span><span class="si">{</span><span class="n">X_next</span><span class="si">}</span><span class="s"> </span><span class="se">\t</span><span class="s"> Balanced Accuracy = </span><span class="si">{</span><span class="n">Y_next</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s"> %</span><span class="sh">"</span><span class="p">)</span>
		
		<span class="c1"># Plots for second iteration onwards 
</span>		<span class="n">B</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
		
		<span class="c1"># add the data to History
</span>		<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="n">X_next</span><span class="p">]]))</span>
		<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="p">[[</span><span class="n">Y_next</span><span class="p">]]))</span>
</code></pre></div></div> <p>Here, I have used a budged to 10 function evaluations in the main loop and 2 function evaluations before the first posterior estimate. An exemplary plot of what comes out at the end is shown below</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/AutoML/plot-iter-3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/AutoML/plot-iter-3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/AutoML/plot-iter-3-1400.webp"></source> <img src="/assets/img/AutoML/plot-iter-3.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The vertical axis is the Balanced Accuracy and the horizontal axis is the learning rate. As can be seen, this is the third iteration of the main loop, with 2 points sampled as an initial estimate, and the acquisition function is the highest at the region with the balance of uncertainty and value of the mean.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/policy-gradients/">Policy Gradients</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/model-free-control/">Model-Free Control</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/model-free-prediction/">Model-Free Prediction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/planning-and-dynamic-programming/">Planning and Dynamic Programming</a> </li> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Aditya Mohan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank"></a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>