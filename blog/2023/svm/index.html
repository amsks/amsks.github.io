<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>State Vector Machines | Aditya Mohan</title> <meta name="author" content="Aditya Mohan"> <meta name="description" content="Personal Website of Aditya Mohan "> <meta name="keywords" content="Reinforcement Learning, Meta-Learning, AutoML, AutoRL"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%A8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://amsks.github.io/blog/2023/svm/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"><span class="font-weight-bold">Aditya </span>Mohan</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Hey there!</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">State Vector Machines</h1> <p class="post-meta">July 5, 2023</p> <p class="post-tags"> <a href="//blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="//blog/category/machine-learning"> <i class="fas fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="intuition">Intuition</h2> <p>Classification is essentially finding a boundary that separates the data points into two segments so that when we get a new data point, the segment that it falls on determines the label it belongs to! The real challenge is positioning this boundary → which is the focus of every classifier. Let’s take the figure shown below as an example</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-1-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-1.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Here, the data is 1D - Mass(g) - and we need to classify the person as Obese (Green) or Not Obese (Red). If we put the decision boundary at the edge of the red points, as shown below, our classifier would work for points inside the respective clusters, but for a point at the boundary, as shown, it would classify it as not obese even though this point is closer to the red cluster.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-2-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-2.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>This is clearly not desirable, and this would also be the case if we move this boundary very close to the green points. Let’s define the shortest distance between the observations and the boundary as the Margin. One way to get a decent classifier would be to place this boundary in such a way so that the margins are equal → This is possible in the case where we put the boundary at exactly half the shortest distance between the data-sets, as shown below</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-3-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-3.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>This is called the Maximum Margin Classifier since the margins, in this case, have the largest value than any other possible case. But the shortfall of this is that it is sensitive to outliers:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-4-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-4.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Here, the outliers push the Max Margins very close to the green cluster and thus, overfit the boundary to the training set as clearly a more equally distributed test set would have to make some misclassifications! Thus, to make a boundary that is more robust to these outliers, we must allow misclassifications by introducing soft margins around our boundary that signify regions in which misclassifications are allowed:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-5-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-5.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Thus, any red data point within this soft margin that falls in the green zone would be classified as green while the data in the red zone would be classified as red. This soft margin, in contrast to the margin above, need not be the shortest distance from the data to the boundary but can be tuned through cross-validation. This classifier is called a <strong>Support Vector Classifier →</strong> The observations within the soft margins are called support vectors which can be tuned. In 2D, the boundary would be a line, in 3D a plane → In general, it is called a hyperplane.</p> <h3 id="non-linearly-seperable-data">Non-linearly seperable data</h3> <p>Now, let us look at new case of drug dosage classification, where the dosage is only safe within a certain range of values and unsafe outside it:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-6-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-6.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Here, the data is not linearly separable since no single boundary can separate it. Thus, support vector classifiers fail here! One way to tackle this is by transforming this data into a higher dimensional plane. If we square each data point and look at the 2D data, then we see that it is now linearly separable!</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-7-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-7.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Now, we can create a boundary in this new higher dimensional plane and just map that back to our original plane. This is called a <strong>Support Vector Machine.</strong> The transformation we performed on the data is called a <strong>Kernel,</strong> and in this case, the Kernel is a \(d^n\) kernel with n having the value of 2 → Polynomial kernel.Technically, the support vector classifier is also an SVM in is dimension.</p> <h2 id="math-of-svm">Math of SVM</h2> <h3 id="hard-margin-svm">Hard Margin SVM</h3> <p>For a set of training data \(\{x_i, y_i\}\), with \(x \in \mathbb{R}^M\) and \(y \in \{-1,+1\}\) with the classifications being -1 and + 1 and \(i = 1, ...., N\) , let us apply a transformation first</p> \[\phi: \mathbb{R}^M \rightarrow \mathbb{R}^D \,\,\, s.t. \,\,\, \phi(x) \in \mathbb{R}^D\] <p>Our objective is to fit a line through this data, and so our model is</p> \[\hat{y}(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x}) + w_0\] <p>which implies that classifications are:</p> <ul> <li>\(y_i(\mathbf{w}^T\phi(x_i) + w_0 ) &gt; 0\) if the classification is correct</li> <li>\(y_i(\mathbf{w}^T\phi(x_i) + w_0 ) &lt; 0\) if the classification is incorrect.</li> </ul> <p>The distance of any training sample from the seperation line line is → \(d(\phi(x_i), L) = \frac{y_i}{\mid \mid \mathbf{w}\mid \mid }(\mathbf{w}^T\phi(x_i) + w_0 )\)</p> <p>Now, our optimization problem is to <strong>maximize the minimum distance for each class,</strong> which can be formalized as</p> \[\underset{\mathbf{w}, w_0}{\text{argmax}} M = \underset{\mathbf{w}, w_0}{\text{argmax}} \{\min_i d(\phi(\mathbf{x_i}),L) = \underset{\mathbf{w}, w_0}{\text{argmax}} \frac{1}{\mid \mid \mathbf{w}\mid \mid } \{ \min_i y_i (\mathbf{w}^T\phi(\mathbf{x_i}) + w_0 ) \}\] <p>Now, we add some constraints to our Margin → Set \(M = 1/\mid \mid \mathbf{w}\mid \mid\) , which essentially means that the minimum distance for our problem has been set to 1 so that we only have to worry about \(\frac{1}{\mid \mid w\mid \mid }\). In other words, we need to have:</p> \[\mid \mathbf{w}^T \phi(\mathbf{x_i})* + w_0\mid = 1\] <p>This is only possible if data points satisfy:</p> \[y_i(\mathbf{w}^T \phi(\mathbf{x_i}) + w_0 ) \geq 1\] <p>If we look at the problem of maximizing \(1/\mid \mid w\mid \mid\), it is the same as minimizing \(\mid \mid w\mid \mid\) → Let’s add two transformations to it :</p> <ol> <li> \[\mid \mid w\mid \mid \rightarrow \mid \mid w\mid \mid ^2\] </li> <li> \[\mid \mid w\mid \mid ^2 \rightarrow \frac{1}{2}\mid \mid w\mid \mid ^2\] </li> </ol> <p>We see our minimization of \(\mid \mid w\mid \mid\) is still satisfied since minimizing half of its square will still give us the same result and so minimizing \(\frac{1}{2}\mid \mid w\mid \mid ^2\) should be the same as maximizing \(\mid \mid w\mid \mid ^{-1}\), which was out the original problem. Thus, we now have to find:</p> \[\min_{\mathbf{w},w_0} \frac{1}{2}\mid \mid \mathbf{w}\mid \mid ^2 \,\,\, s.t. \,\,\, y_i(\mathbf{w}^T\phi(\mathbf{x_i}) + w_0 ) \geq 1 \,\,\,\, \forall \,\,\,\, i = 1, ...., N\] <p>This is called the <strong>primal form</strong> → the form we reached using our intuition. We had a maximization type optimization and we converted it to a minimization problem without messing with our original goal. We now have to solve a constrained minimization problem. We solve this by creating a Langrangian Formulation → a function of the form:</p> \[L(x,y,\lambda) = f(x,y) - \sum \lambda_i g_i(x,y)\] <p>where f is our optimization target and we put constraints on it in the form of \(g_i\) which are \(\lambda_i\) is the Lagrange multipliers. Our approach to solving this is to differentiate the Lagrangian w.r.t the variables to get critical points</p> \[\nabla L(x,y,\lambda) = 0 \longrightarrow \nabla f(x,y) = \lambda \nabla g(x,y)\] <p>and substitute it back into \(L\) to get a <strong>Dual Formulation</strong>. So, formalizing our SVM minimization as a Lagrangian with \(\alpha\) as our multiplier, we get:</p> \[L(\mathbf{w}, w_0, \mathbf{\alpha}) = \frac{1}{2}\mid \mid \mathbf{w}\mid \mid ^2 - \sum_{i=1}^{N}\alpha_i [y_i(\mathbf{w}^T \phi(\mathbf{x_i}) + w_0 ) - 1]\] <p>To minimize this function, we first differentiate L w.r.t \(\mathbf{w}\) :</p> \[\begin{aligned} &amp;\partial L/\partial \mathbf{w} = \mathbf{w} - \sum\alpha_iy_i \phi(\mathbf{x_i}) = 0 \\ \implies &amp;\mathbf{w} = \sum\alpha_iy_i \phi(\mathbf{x_i}) \\ \end{aligned}\] <p>Then, we differentiate L w.r.t \(w_0\) as follows:</p> \[\begin{aligned} &amp; \partial L/\partial w_0 = - \sum\alpha_iy_i = 0\\ \implies &amp;\sum \alpha_iy_i = 0 \\ \end{aligned}\] <p>Then we differentiate w.r.t w :</p> \[\begin{aligned} &amp; \partial L/\partial \mathbf{w} = \mathbf{w} - \sum\alpha_iy_i\phi(x_i) = 0 \\ \implies &amp; \mathbf{w} = \sum \alpha_iy_i\phi(x_i) \\ \end{aligned}\] <p>We now plug the values into our lagrangian:</p> \[\begin{aligned} &amp;\frac{1}{2}\mid \mid \mathbf{w}\mid \mid ^2 - \sum_{i=1}^{N}\alpha_i [y_i(\mathbf{w}^T \phi(\mathbf{x_i}) + w_0 ) - 1] \\ &amp;= \frac{1}{2}(\sum\alpha_iy_i \phi(\mathbf{x_i}) \sum\alpha_jy_j\phi(\mathbf{x_j})) - (\sum\alpha_iy_i\phi(\mathbf{x_i}) \sum\alpha_jy_j \phi(\mathbf{x_j})) - (\sum \alpha_iy_iw_0) + \sum \alpha_i \\ \therefore \,\,\, &amp; L(\mathbf{\alpha}) = \sum \alpha_i - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j [\phi(\mathbf{x_i}) . \phi(\mathbf{x_j})] \\ \end{aligned}\] <p>The new Expression → \(L(\mathbf{\alpha}) = \sum \alpha_i - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j [\phi(\mathbf{x_i}) . \phi(\mathbf{x_j})]\) is the <strong>Dual Form</strong> of the Maximum Margin Problem. An important thing to notice is that our dual form only depends on the dot products of our data points and thus, we can take advantage of Kernelization to make this independent of the nature of \(\phi\) :</p> \[L(\mathbf{\alpha}) = \sum \alpha_i - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j K(\mathbf{x_i}.\mathbf{x_j})\] <p>And for classifying any new point, all we have to do it replace it in the place of \(x_j\) as shown:</p> \[\hat{y}_{test} = \sum_{i=1}^{N} \hat{\alpha_i} [y_i(\phi(\mathbf{x_i}).\phi(\mathbf{x_{test}}) + w_0 )]\] <p>and this should satisfy the Karush-Kuhn-tucker conditions:</p> <ol> <li> \[\alpha_i \geq 0\] </li> <li> \[y_i(\mathbf{w}^T\phi(\mathbf{x_i}) + w_0 ) - 1 &gt; 0\] </li> <li> \[\alpha_i [y_i(\mathbf{w}^T\phi(\mathbf{x_i}) + w_0 ) - 1] = 0\] </li> </ol> <p>This is the key to SVMs → If \(\alpha = 0\) then the point is not contributing to the margin and is not a support vector and for all \(\alpha &gt; 0\) , the point \(\mathbf{x}\) is a support vector and contributes to the margin.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-8-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-8-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-8-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-8.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Thus, in summary, the dual expression gives us a set of \(\alpha_i\) that represent points that are either support vectors or not. If we want to find the best boundary for these hard margins, all we have to do is compute</p> \[\mathbf{\hat{w}} = \sum \hat{\alpha_i} y_i \phi(\mathbf{x_i})\] <p>Which we then substitute into the third KKT condition, t get \(w_0\) as follows:</p> \[\begin{aligned} &amp;\alpha_i [y_i(\mathbf{\hat{w}}^T\phi(\mathbf{x_i}) + w_0 ) - 1] = 0 \\ \implies &amp;y_i(\mathbf{\hat{w}}^T\phi(\mathbf{x_i}) + w_0 ) = 1 \\ \end{aligned}\] <p>Now, the y labels are either +1 or -1, and so, \(w_0 = 1 - \mathbf{\hat{w}}^T\phi(\mathbf{x_i})\) or \(w_0 = -1 - \mathbf{\hat{w}}^T\phi(\mathbf{x_i})\) , which we can also write as:</p> \[w_0 = y_i - \mathbf{\hat{w}}^T\phi(\mathbf{x_i})\] <p>In practice, we compute \(w_0\) by summing multiple such expressions and dividing by \(\alpha_i\). A new point will be classified to</p> <ul> <li>Class 1 if \(\mathbf{\hat{w}}^T\phi(\mathbf{x_i}) + \hat{w}_0 &gt; 0\)</li> <li>Class 2 if \(\mathbf{\hat{w}}^T\phi(\mathbf{x_i}) + \hat{w}_0 &lt; 0\)</li> </ul> <h3 id="soft-margin-svm">Soft-Margin SVM</h3> <p>The hard-margin constraint is very strong but does not work very well for overlapping classes or spread out data. Thus, we relax the constraints by allowing points to violate the margins → this is quantified by the slack variable \(\xi_i \geq 0\) for each point i, which measures the extent of violation.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img//MALIS/SVM/svm-9-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img//MALIS/SVM/svm-9-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img//MALIS/SVM/svm-9-1400.webp"></source> <img src="/assets/img//MALIS/SVM/svm-9.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Thus, for point outside the margins \(\xi_i = 0\) and for points inside the margin \(\xi_i = \mid y_i - \hat(y(\mathbf{x_i})\mid\), and now, our constraint becomes</p> \[y_i(\mathbf{w}^T\mathbf{x_i} + w_0 ) \geq 1 - \xi_i\] <p>And so, our problem becomes:</p> \[\min_{\mathbf{w},w_0, \mathbf{\xi}} C \sum \xi_i + \frac{1}{2} \mid \mid w\mid \mid ^2 \,\,\,\, s.t \,\,\,\, y_i(\mathbf{w}^T\phi(\mathbf{x_i}) + w_0 ) \geq 1 - \xi_i \,\,\,\, \xi_i \geq 0 \,\,\, \forall n\] <p>We again formulate the Lagrangian:</p> \[L(\mathbf{w}, w_0, \mathbf{\xi}, \mathbf{\alpha}, \mathbf{\lambda}) = = \{\frac{1}{2}\mid \mid \mathbf{w}\mid \mid ^2 + C\sum_i \xi_i \}- \sum_{i=1}^{N}\alpha_i [1 - \xi_i - y_i(\mathbf{w}^T \phi(\mathbf{x_i}) + w_0 )] + \sum_i \lambda_i(-\xi_i)\] <p>Which, when differentiated gives the follownig expresions:</p> \[\begin{aligned} &amp;\mathbf{w} = \sum\alpha_iy_i \phi(\mathbf{x_i})\\ &amp;\sum \alpha_iy_i = 0 \\ &amp;C - \alpha_i - \lambda_i = 0 \end{aligned}\] <p>The interesting thing is that the third expression helps us eliminate \(\xi_i\) from the Dual form to get the expression:</p> \[\sum \alpha_i - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j \phi(\mathbf(x_i)).\phi(\mathbf(x_j))\] <p>Which is the same as before. Thus, we can see that the only impact \(\xi\) has is in shifting translating the constraint to include misclassifications. Thus, soft-margins allow us to perform the same optimization with an added impact on the classification constraint.</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Aditya Mohan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank"></a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>