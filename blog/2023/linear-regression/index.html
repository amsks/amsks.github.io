<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Linear Regression | Aditya Mohan</title> <meta name="author" content="Aditya Mohan"> <meta name="description" content="Hello, Hello, Hello! We are gathered here today to get through this thing called life. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://amsks.github.io/blog/2023/linear-regression/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"><span class="font-weight-bold">Aditya </span>Mohan</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Namaste!</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching and theses</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Regression</h1> <p class="post-meta">July 5, 2023</p> <p class="post-tags"> <a href="//blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="//blog/category/machine-learning"> <i class="fas fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Main ideas :</strong></p> <ul> <li>Use Least squares to fit a line to Data</li> <li>Use R square</li> <li>Use p value</li> </ul> <p><strong>Fitting the Line →</strong> Try to minimize a metric that represents the fit</p> <ul> <li>Let the Line be \(y(x) = w_0 + w_1x\)</li> <li>Now, our optimization goal is to find the values of \(w_0, w_1\) so that the variation around this line is minimal → We do this by minimizing the squared Errors</li> </ul> <p>To know if taking into the samples actually improves anything or not, all we have to do is calculate the variance around the fit and compare it with variance around the mean of the y values of the point, and give an answer in percentages! This is called the \(R^2\) value:</p> \[R^2 = \frac {\text{Var}(mean) - \text{Var}(fit)}{\text{Var}(\text{mean})}\] <p>Thus, if this value is 0.6 , we get a 60% improvement in the variance by taking the x features into account.</p> <p>Let’s go to the interesting stuff → The Math of this all</p> <h2 id="math-of-regression">Math of Regression</h2> <p>Let’s take the case of a set of multidimensional features \(\mathbf{X} \in \mathbb{R}^D \,\,\,\) where, \(i= 1,...,N\). For each of these set of D dimensional inputs, we have one output \(\mathbf{y} \in \mathbb{R}\). Thus, we have our data as ${(X_i,y_i)}$ to which we have to fit a D dimensional hyperplane so that the variance around this hyperplane is minimal. Let’s start by defining our model:</p> \[\begin{aligned} &amp;y_i(X_i) = f(X_i) + \epsilon \\ &amp;\hat{y_i} = \hat{f}(X_i) \\ \end{aligned}\] <p>Here, the actual data is a function \(f: \mathbb{R}^D \mathbb{R}ightarrow \mathbb{R}\) and our hyperplane is function \(\hat{f}: \mathbb{R}^D \mathbb{R}ightarrow \mathbb{R}\) which produces the targets \(y\) and prediction \(\hat{y}\), respectively. So, we can check the error between our actual data and the predicted data, which we call the Sum-of-square error:</p> \[\begin{aligned} &amp;\mathbf{e} = (\mathbf{y} - \mathbf{\hat{y}})^2 \\ &amp;\mathbf{e} = (\mathbf{y} - \mathbf{\hat{y}})^T(\mathbf{y} - \mathbf{\hat{y}})\\ \end{aligned}\] <p>Here, I have used bold to represent vector notation. since our model is linear, we can define it as:</p> \[\hat{f}(\mathbf{X}) = \mathbf{X}\mathbf{w}\] <ul> <li> <strong>Note:</strong> to make this work by taking bias into account we let \(\mathbf{w} \in \mathbb{R}^{D+1}\) where the D weights are corresponding to D features and the extra weight is the bias. Thus, \(\mathbf{X} \in \mathbb{R}^{NX(D+1)}\) which basically means that our N observations are stacked vertically and each observation is of D dimensions, but to make the notation work, we add a 1 at the start, which will be the multiplier for our bias term, and thus, have D+1 as the dimension of the row.</li> </ul> <p>Thus, our error now becomes</p> \[\begin{aligned} &amp;\mathbf{e} = (\mathbf{y} -\mathbf{X}\mathbf{w} )^T(\mathbf{y} - \mathbf{X}\mathbf{w}) \\ \implies &amp;\mathbf{e} = (\mathbf{y} -\mathbf{w}^T\mathbf{X}^T )(\mathbf{y} - \mathbf{X}\mathbf{w}) \\ \implies &amp;\mathbf{e} = \mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\mathbf{w} - \mathbf{w}^T\mathbf{X}^T\mathbf{y} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w} \\ \end{aligned}\] <p>Now, to get our optimal weights we follow the method to get the minima of e i.e differentiate e w.r.t \(\mathbf{w}\) and then set it to 0:</p> \[\begin{aligned} &amp;\nabla_w\mathbf{e} = 0 \\ \implies &amp;\nabla_w(\mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\mathbf{w} - \mathbf{w}^T\mathbf{X}^T\mathbf{y} + \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w} ) = 0 \\ \implies &amp;\nabla_w(\mathbf{y}^T\mathbf{y}) - \nabla_w(\mathbf{y}^T\mathbf{X}\mathbf{w}) - \nabla_w(\mathbf{w}^T\mathbf{X}^T\mathbf{y}) + \nabla_w(\mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}) = 0 \\ \implies &amp;-2\mathbf{y}^T\mathbf{X} - 2\mathbf{w}^T\mathbf{X}^T\mathbf{X} = 0 \\ \implies &amp;(\mathbf{X}^T\mathbf{X})\mathbf{w}^T = \mathbf{y}^T\mathbf{X} \\ \therefore \,\, &amp;\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{y}\\ \end{aligned}\] <p>Hence, all we need to do is plug-in \(\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{y}\) into our original equation and we get the solution. Of course, this is the optimization variant of our regression problem and gradient descent goes around this by computing this solution iteratively by taking an initial guess of \mathbf{w} and then moving towards the direction of decrease, and moving proportionally to the rate of decrease. However, the solution to which it should end up converging is the same! We can also do all sorts of gymnastics around this solution to make the variance go down even further. For example, we could transform our input \(\mathbf{X}\) to a new space by \(\mathbf{\phi(\mathbf{X})}\), in which subject to 1-1 mapping, our solution would simply become</p> \[\mathbf{w} = (\mathbf{\phi(\mathbf{X})}^T\mathbf{\phi(\mathbf{X})})^{-1} \mathbf{\phi(\mathbf{X})}^T\mathbf{y}\] <p>The essence of regression remains the same. in the case where \(D= 2\), we use this same technique on 2D matrices and those simplistic equation for the starting points of regression that we see in most places.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Aditya Mohan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank"></a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>