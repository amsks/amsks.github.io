<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://amsks.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://amsks.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-26T07:12:38+00:00</updated><id>https://amsks.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal Website of Aditya Mohan </subtitle><entry><title type="html">Model-Free Control</title><link href="https://amsks.github.io/blog/2023/model-free-control/" rel="alternate" type="text/html" title="Model-Free Control"/><published>2023-07-10T16:57:00+00:00</published><updated>2023-07-10T16:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/model-free-control</id><content type="html" xml:base="https://amsks.github.io/blog/2023/model-free-control/"><![CDATA[<p>While prediction is all about estimating the value function in an environment for which the underlying MDP is not known, Model-Free control deals with optimizing the value function. While many problems can be modelled as MDPs, in a lot of problems we don’t really have that liberty in some sense. The reasons why using an MDP to model the problem might not make sense are:</p> <ul> <li>MDP is unknown → In this case we have to sample experiences and somehow work with samples.</li> <li>MDP is known, but too complicated in terms of space and so, we again have to rely on experience</li> </ul> <p>We can classify the policy learning process into two kinds based on the policy we learn and the policy we evaluate upon:</p> <ul> <li><strong>On-Policy Learning</strong> → If we learn about policy \(\pi\) from the experiences sampled from \(\pi\) , then we are essentially learning on the job.</li> <li><strong>Off-Policy Learning</strong> → If we use a policy \(\mu\) to sample the experiences, but our target is to learn about policy \(\pi\) , then we are essentially seeing someone else do something and learning how to do something else through that</li> </ul> <h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2> <p>As explained before our process of learning can also be broken down into 2 stages:</p> <ol> <li><strong>Policy Evaluation</strong> → Iteratively estimating \(v_\pi\) throught the samled experiences a.k.a iterative policy evaluation</li> <li><strong>Policy Improvement →</strong> Generating a policy \(\pi' \geq \pi\)</li> </ol> <p>The process of learning oscillates between these two states in sense → We evaluate a policy, then improve it, then evaluate it again and so on until we get the optimal policy $\pi^*$ and the corresponding optimal value \(v^*\). Thus, we could see this as state transitions, as shown below:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-1-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-1.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>A really good way to look at convergence is shown below:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-2-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/Model-Free-Control/mfc-2.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Each process drives the value function or policy toward one of the lines representing a solution to one of the two goals. The goals interact because the two lines are not orthogonal. Driving directly toward one goal causes some movement away from the other goal. Inevitably, however, the joint process is brought closer to the overall goal of optimality. The arrows in this diagram correspond to the behavior of policy iteration in that each takes the system all the way to achieving one of the two goals completely. In GPI one could also take smaller, incomplete steps toward each goal. In either case, the two processes together achieve the overall goal of optimality even though neither is attempting to achieve it directly. Thus, almost all the stuff in RL can be described as a GPI, since this oscillation forms the core of it, and as mentioned before the ideal solution is usually out of our reach due to computational limitations, and DP has its set of disadvantages.</p> <h2 id="on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</h2> <p>Our iteration and evaluation steps are:</p> <ul> <li>We use Monte-Carlo to estimate the value</li> <li>We use greedy policy improvement to get a better policy</li> </ul> <p>Ideally, we can easily use the value function \(v_\pi\) for evaluation and then improve upon it. However, if we look at the improvement step using \(v_\pi\) , w have the equation</p> \[\pi'(s) = \argmax_{a \in \mathcal{A}} R^a_s + P_{ss'}^a V(s')\] <p>Here, to get the \(P_{ss'}^a\) we need to have a transition model, which goes against our target of staying model-free. The action-value \(q_\pi\), on the other hand, does not require this transition probability:</p> \[\pi'(s) = \argmax_{a \in A} Q(s,a)\] <p>Thus, using \(q_\pi\) allows us to close the loop in a model-free way. Hence, we now have our 2-step process that needs to be repeated until convergence as:</p> <ul> <li>Iteratively Evaluate \(q_\pi\) using Monte-Carlo methods</li> <li>Improve to \(\pi' \geq \pi\) greedily</li> </ul> <h2 id="maintaining-exploration-and-stochastic-strategies">Maintaining Exploration and Stochastic Strategies</h2> <p>Greedy improvement is essentially asking us to select the policy that leads to the best value based on the immediate value that we see. This suffers from the problem of maintaining exploration since many relevant state-action pairs may never be visited → If \(\pi\) is a deterministic policy, then in following \(\pi\) one will observe returns only for one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. Hence, we need to ensure continuous exploration. One way to do this is by specifying that the first step of each episode starts at a state-action pair and that every such pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times within the limit of an infinite number of episodes. This is called the assumption of <strong>exploring starts</strong>. However, it cannot be relied upon in general, particularly when learning directly from real interactions with an environment. Thus, we need to look at policies that are stochastic in nature, with a nonzero probability of selecting all actions.</p> <h3 id="epsilon-greedy-strategy">\(\epsilon\)-greedy Strategy</h3> <p>One way to make deterministic greedy policies stochastic is to follow an \(\epsilon\)-<strong>greedy strategy</strong> → We try all \(m\) actions with non-zero probability, and choose random actions with a probability \(\epsilon\), while maintaining a probability of \(1- \epsilon\) for choosing actions based on the greedy evaluation. Thus, by controlling \(\epsilon\) as a hyperparameter, we tune how much randomness our agent is willing to accept in its decision:</p> \[\pi(a|s) = \begin{cases} \frac{\epsilon}{m} + 1 - \epsilon \,\,\,\,\,\,\,\, if \,\,\, a^* = \argmax_{a \in A} Q(s,a) \\ \frac{\epsilon}{m} \,\,\,\,\,\,\,\, otherwise \end{cases}\] <p>The good thing is that we can prove that the new policy that we get with the \(\epsilon\)-greedy strategy actually does lead to a better policy:</p> \[\begin{aligned} q_\pi(s, \pi'(s)) &amp; = \sum_{a \in \mathcal{A}} \pi'(a|s) q_\pi(s,a) \\ &amp; = \frac{\epsilon}{m} \sum_{a \in \mathcal{A}} \pi'(a|s) q_\pi(s,a) + ( 1- \epsilon) \max_{a \in \mathcal{A} } q_\pi(s,a) \\ &amp; \geq \frac{\epsilon}{m} \sum_{a \in \mathcal{A}} \pi'(a|s) q_\pi(s,a) + ( 1- \epsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a|s) - \frac{\epsilon}{m}}{1 - \epsilon} q_\pi(s,a) \\ &amp; = v_\pi (s) \\ \therefore v_\pi(s') &amp; \geq v_\pi (s) \end{aligned}\] <h3 id="glie">GLIE</h3> <p>Greedy in the Limit with Infinite Exploration, as the name suggests, is a strategy in which we are essentially trying to explore infinitely, but reducing the magnitude of exploration over time so that in the limit the strategy remains greedy. Thus, a GLIE strategy has to satisfy 2 conditions:</p> <ul> <li> <p>If a state is visited infinitely often, then each action in that state is chosen infinitely often</p> \[\lim_{k \rightarrow \infty } N_k(s,a) = \infty\] </li> <li> <p>In the limit, the learning policy is greedy with respect to the learned Q-function</p> \[\lim_{k \rightarrow \infty} \pi_k(a|s) = \bm{1} (a = \argmax_{a \in \mathcal{A}} Q_k(s, a'))\] </li> </ul> <p>So, to convert our \(\epsilon\)-greedy strategy to a GLIE strategy, for example, we need to ensure that the magnitude of \(\epsilon\) decays overtime to 0. Two variants of exploration strategies that have been shown to be GLIE are:</p> <ul> <li>\(\epsilon\)-greedy with exploration with \(\epsilon_t = c/ N(t)\) where \(N(t)\) → number of visits to state \(s_t=s\)</li> <li> <p>Boltzmann Exploration with</p> \[\begin{aligned} &amp; \beta_t(s) = \log (\frac{n_t(S)}{C_t(s)} ) \\ &amp; C_t(s) \geq \max_{a,a'}|Q_t(s,a) - Q_t(s,a')| \\ &amp; P(a_t = a | s_t = s ) = \frac{exp(\beta_t(s) Q_t(s,a))}{\sum_{b\in \mathcal{A}} exp(\beta_t(s) Q_t(s,a))} \end{aligned}\] </li> </ul> <h3 id="glie-monte-carlo-control">GLIE Monte-Carlo Control</h3> <p>We use Monte-Carlo estimation to get an estimate of the policy and then improve it in a GLIE manner as follows:</p> <ul> <li>Sample k\(^{th}\) episode using policy $\pi$ so that \(\{S_1, A_1, R_1, ....., S_T \} \sim \pi\)</li> <li> <p>For each state and action, update the Number of visitation</p> \[\begin{aligned} &amp; N(S_t, A_t) \leftarrow N(S_t, A_t) + 1 \\ &amp; Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t) ) \end{aligned}\] </li> <li> <p>Improve the policy based on action value as</p> \[\begin{aligned} &amp; \epsilon \leftarrow \frac{\epsilon}{k} \\ &amp; \pi \leftarrow \epsilon-greedy(Q) \end{aligned}\] </li> </ul> <p>Thus, now we update the quality of each state-action pair by running an episode and update all the states that were encountered. Then we lip a coin with \(1- \epsilon\) probability of selecting the state-action pair with the highest \(Q(s,a)\) from all the possible choices and \(\epsilon\) probability of taking a random pair. Hence, we are able to ensure exploration happens with an $\epsilon$ probability, and this probability changes proportional to \(\frac{1}{k}\) which essentially allows us to get more greedy as \(k\) increases, with the hope that we converge to the optimal policy. In fact, it has</p> <h2 id="off-policy-monte-carlo-control">Off-Policy Monte-Carlo Control</h2> <p>To be able to use experiences sampled from a policy \(\pi'\) to estimate \(v_\pi\) or \(q_\pi\), we need to understand how the policies might relate to each other. To be even able to make a comparison, we first need to ensure that every action taken under \(\pi\) is also taken, at least occasionally, under \(\pi'\) → This allows us to guarantee representation of actions being common between the policies and thus, we need to ensure</p> \[\pi(s,a) &gt; 0 \implies \pi'(s,a) &gt; 0\] <p>Now, let’s consider we have the $i^{th}$ visit to a state $s$ in the episodes generated from \(\pi'\) and the sequence of states and actions following this visit and let \(P_i(s), P_i'(s)\) denote the probabilities of that complete sequence happening given policies \(\pi, \pi'\) and let \(R_i(s)\) be the return of this state. Thus to estimate \(v_\pi(s)\) we only need to weigh the relative probabilities of \(s\) happening in both policies. Thus, the desired MC estimate after \(n_s\) returns from \(s\) is:</p> \[V(s) = \frac{\sum_{i=1}^{n_s} \frac{P_i(s)}{P_i(s')} R_i(s)}{\sum_{i=1}^{n_s} \frac{P_i(s)}{P_i(s')}}\] <p>We know that the probabilities are proportiona to the transition probabilities in each policy and so</p> \[P_i(s_t) = \prod_{k=t}^{T_i(s) - 1} \pi(s_k, a_k) \mathcal{P}^{a_k}_{s_k s_{k+1}}\] <p>Thus, when we take the ratio, we get</p> \[\frac{P_i(s)}{P_i'(s)} = \prod_{k=t}^{T_i(s) - 1} \frac{\pi(s_k, a_k)}{\pi'(s_k, a_k)}\] <p>Thus, we see that the weights needed to estimate \(V(s)\) only depend on policies, and not on the dynamics of the environment. This is what allows off-policy learning to be possible. The advantage this gives us is that the policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the estimation policy. Thus, the estimation policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions!!</p> <h3 id="importance-sampling">Importance Sampling</h3> <p>In MC off-policy control, we can use the returns generated from policy \(\mu\) to estimate policy \(\pi\) by weighing the target \(G_t\) based on the similarity between the policies. This is the essence of importance sampling, where we estimate the expectation of a different distribution based on a given distribution:</p> \[\begin{aligned} \mathbb{E}_{X \sim P}[f(X)] &amp; = \sum P(X) f(X) \\ &amp; = \sum Q(X) \frac{P(X)}{Q(X)}f(X) \\ &amp; = \mathbb{E}_{X \sim Q} \bigg[ \frac{P(X)}{Q(X} f(X) \bigg] \end{aligned}\] <p>Thus, we just multiple te importance sampling correlations along the episodes and get:</p> \[G^{\pi/\mu}_t = \frac{\pi(a_t|s_t)}{\mu(a_t|s_t)} \frac{\pi(a_{t+1}|s_{t+1})}{\mu(a_{t+1}|s_{t+1})} ... \frac{\pi(a_T|s_T)}{\mu(a_T|s_T)} G_t\] <p>And now, we can use \(G^{\pi/\mu}_t\) to compute our value update for MC-control.</p> <h2 id="td-policy-control">TD-Policy Control</h2> <p>The advantages of TD-Learning over MC methods are clear:</p> <ol> <li>Lower variance</li> <li>Online</li> <li>Incomplete sequences</li> </ol> <p>We again follow the pattern of GPI strategy, but this time using the TD estimate of the target and then again encounter the same issue of maintaining exploration, which leads us to on-policy ad off-policy control. As was the case with MC control, we need to remain model-free and so we shift the TD from estimating state-values to action-values. We know that formally, they both are equivalent and essentially Markov chains.</p> <h3 id="on-policy-control--sarsa">On-Policy Control : SARSA</h3> <p>We can use the same TD-target as state values to get the update for state-action pairs:</p> \[Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[ R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \big]\] <p>This is essentially operating over the set of current states and action, one step look-ahead of the same values and the reward of the next pair → \((s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\) - and the order in which this is written is <strong>S</strong>tate → <strong>A</strong>ction → <strong>R</strong>ewards → <strong>S</strong>tate → <strong>A</strong>ction. Thus, this algorithm is called SARSA. We can use SARSA for evaluating our policies and then improve the policies, again, in an $\epsilon$-greedy manner. SARSA converges to \(q^*(s,a)\) under the following conditions:</p> <ul> <li> <table> <tbody> <tr> <td>GLIE sequences of policies $$\pi_t(a</td> <td>s)$$</td> </tr> </tbody> </table> </li> <li> <p>Robbins-Monro sequence of step-sizes \(\alpha_t\)</p> \[\begin{aligned} &amp; \sum_{t=1}^\infty \alpha_t = 0 \\ &amp; \sum_{t=1}^\infty \alpha^2_t &lt; \infty \end{aligned}\] </li> </ul> <p>We can perform a similar modification on SARSA to to extend it to n-steps by defining a target based on n-step returns:</p> \[\begin{aligned} &amp; q_t^{(n)} = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... + \gamma^{n-1} r_{t+n} + \gamma^n Q(s_{t+n}) \\ \therefore \,\,\,&amp; Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[ q_t^{(n)} - Q(s_t, a_t) \big] \end{aligned}\] <p>Additionally, we can also formulate a forward-view SARSA(\(\lambda\)) by combining n-step returns:</p> \[q_t^\lambda = (1 - \lambda) \sum _{n=1}^\infty \lambda ^{n-1} q_t^{(n)}\] <p>and just like TD(\(\lambda\)), we can implement Eligibility traces in online algorithms, in which case there will be one eligibility trace for each state-action pair:</p> \[\begin{aligned} &amp; E_0(s,a) = 0 \\ &amp; E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \bm{1}(s,a) \\ \therefore \,\,\, &amp; Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \, E_t(s,a) \big[ q_t^{(n)} - Q(s_t, a_t) \big] \end{aligned}\] <h3 id="td-off-policy-learning--q-learning">TD Off-Policy Learning : Q-Learning</h3> <p>For off-policy learning in TD, we can again look at the relative weights and use importance sampling. However, since the lookahead is only one-step and not n-step sequence sampling, we only need a single importance sampling correction to get</p> \[V(s_t) \leftarrow V(s_t) + \alpha \, \bigg( \frac{\pi(a_t|s_t)}{\mu(a_t|s_t)} (R_{t+1} + \gamma \, V(s_{t+1})) - V(s_t) \bigg)\] <p>The obvious advantage is the requirement of only a one-step correlation, which leads to much lower variance. One of the most important breakthroughs in reinforcement learning was the development of Q-learning, which does not require importance sampling. The simple idea that makes the difference is:</p> <ul> <li>we choose our next action from the behavior policy i.e \(a_{t+1} \sim \mu\) BUT we use the alternative action sampled from the target policy \(a' \sim \pi\) to update towards. Thus, our equation becomes:</li> </ul> \[Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[ R_{t+1} + \gamma Q(s_{t+1}, a'_{t+1}) - Q(s_t, a_t) \big]\] <ul> <li> <table> <tbody> <tr> <td>Then, we allow both behavior and target policies to improve → This means that $$\pi(s_{t+1}</td> <td>a_{t+1} ) = \argmax_{a’} Q(s_{t+1}, a’)\(while\)\mu(s_{t+1}</td> <td>a_{t+1} ) = \argmax_{a} Q(s_{t+1}, a)$$ . Thus, our final equation simplifies to:</td> </tr> </tbody> </table> \[Q(s, a) \leftarrow Q(s, a) + \alpha \big[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \big]\] </li> </ul> <p>This dramatically simplifies the analysis of the algorithm. We can see that all that is required for correct convergence is that all pairs continue to be updated.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[While prediction is all about estimating the value function in an environment for which the underlying MDP is not known, Model-Free control deals with optimizing the value function. While many problems can be modelled as MDPs, in a lot of problems we don’t really have that liberty in some sense. The reasons why using an MDP to model the problem might not make sense are:]]></summary></entry><entry><title type="html">Policy Gradients</title><link href="https://amsks.github.io/blog/2023/policy-gradients/" rel="alternate" type="text/html" title="Policy Gradients"/><published>2023-07-10T16:57:00+00:00</published><updated>2023-07-10T16:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/policy-gradients</id><content type="html" xml:base="https://amsks.github.io/blog/2023/policy-gradients/"><![CDATA[<p>The core idea of Reinforcement learning is to learn some kind of behavior through optimizing for rewards. The behavior learned by an agent i.e. the schema it follows while going through this process is the learned policy that it uses to decide which action to take and thus, the transition from one state to another. One way to close the loop for the agent to learn is by evaluating the states and actions through value functions and thus, our way to measure the learned policy is seen through these value functions, approximated by lookup tables, linear combinations, Neural Networks e.t.c. Policy Gradient methods take a different approach where they bypass the need for a value function by parameterizing the policy directly. While the agent can still use a value function to learn, it need not use it for selecting actions. The advantages that policy gradient methods offer are 3 fold:</p> <ol> <li>Approximating the policy might be simpler than approximating action values and a policy-based method might typically learn faster and yield a superior asymptotic policy. One very good example that illustrates this is the work by <a href="http://proceedings.mlr.press/v48/simsek16.pdf">Simsek et. al</a> on the game of Tetris where they showed that it is possible to choose amongst actions without really evaluating them.</li> <li>Policy gradient methods can handle stochastic policies. The case of card games with imperfect information, like poker, is a direct example where the optimal play might be to do 2 different things with certain probabilities. If we are maximizing the actions based on value approximations, we don’t really have a natural way of finding stochastic policies. Policy Gradient methods can do this.</li> <li>Policy gradient methods offer stronger convergence guarantees since with continuous policies the action probabilities change smoothly. This is not the case with the fixed \(\epsilon\) -greedy evaluation since there is always a probability to do something random.</li> <li>The choice of policy parameterization is sometimes a good way of injecting prior knowledge about a desired form of the policy into the system. This is especially helpful when we look at introducing Meta-Learning strategies into Reinforcement Learning.</li> </ol> <p>In the following sections, I first use the theoretical treatment done in Sutton and Barto’s book since I was better able to understand policy gradients’ essence through this. However, I again do the derivation by looking at the whole thing from the viewpoint of trajectories, since I find it more intuitive.</p> <h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2> <p>The issue with the parameterization of the policy is that the policy affects both the action selections and the distribution of states in which those selections are made. While going from state to action is straightforward, going the other way round involves the environment and thus, the parameterization is typically unknown. Thus, with this unknown effect of policy changes on the state distributions, the issue is evaluating the gradients of the performance. This is where the policy gradient theorem comes into the picture, as it shows that the gradient of the policy w.r.t its parameters does not involve the derivative of the state distribution. For episodic tasks, if we assume that every episode starts in some particular state \(s_0\), then we can write a performance measure as the value of the start state of the episode</p> \[J(\mathbf{\theta}) = v_{\pi_\theta} (s_0)\] <p>For simplicity, we remove the \(\bf{\theta}\) from the subscript of \(\pi\). Now, to get a derivative of this measure, we start by differentiating this value function w.r.t \(\mathbf{\theta}\):</p> \[\begin{aligned} \nabla_{\mathbf{\theta}} J(\mathbf{\theta}) &amp; = \nabla v_\pi (s) \\ &amp; = \nabla \bigg [ \sum_{a \in \mathcal{A}}\pi(a|s) q_\pi(s,a) \bigg ] \\ &amp; = \sum_a \bigg[ \nabla\pi(a|s) q_\pi (s, a) + \pi(a|s) \nabla q_\pi(s,a) \bigg ] \\ &amp; = \sum_a \bigg[ \nabla\pi(a|s) q_\pi (s, a) + \pi(a|s) \,\, \nabla \sum _{s' \in \mathcal{S}, r \in \mathcal{R}} p(s', r | s, a) ( r + v_\pi (s') ) \bigg ] \end{aligned}\] <p>we now extend \(q_\pi (s,a)\) in the second term on the right to the rollout for the new state \(s'\).</p> \[\nabla v_\pi (s) = \sum_a \bigg[ \nabla\pi(a|s) q_\pi (s, a) + \pi(a|s) \,\, \nabla \sum _{s' \in \mathcal{S}, r \in \mathcal{R}} p(s', r | s, a) ( r + v_\pi (s') ) \bigg ]\] <p>The reward is independent of the parameters $\theta$, so we can set that derivative inside the sum to 0, and so, we get:</p> \[\nabla v_\pi (s) = \sum_a \bigg[ \nabla\pi(a|s) q_\pi (s, a) + \pi(a|s) \,\, \nabla \sum _{s' \in \mathcal{S} } p(s' | s, a) v_\pi (s') \bigg ]\] <p>Thus, we now have a recursive formulation of \(\nabla v_\pi (s)\) in terms of \(\nabla v_\pi (s')\). To calculate this derivative in the infinite horizon episodic case we just need to unroll this infinitely many times, which can be written as</p> \[\sum_{x \in \mathcal{s}} \sum _{k=0}^\infty P(s \rightarrow x, k , \pi ) \sum_{a \in \mathcal{A}} \nabla \pi (a|s) q_\pi (x, a)\] <p>Here, \(P(s \rightarrow x, k, \pi )\) is the probability of transitioning from state \(s\) to state \(x\) in \(k\) steps under policy \(\pi\). To estimate this probability we use something called the stationary distribution of the Markov chains. This term comes from the <a href="http://www.math.uchicago.edu/~may/VIGRE/VIGRE2008/REUPapers/Plavnick.pdf">Fundamental Theorem of Markov Chains</a> which intuitively says that in very long random walks the probability of ending up at some state is independent of where you started. When we club all these probabilities into a distribution over the states, then we have a stationary distribution, denoted by \(\mu(s)\). In on-policy training, we usually estimate this distribution by the fraction of time spent in a state. In our case of episodic tasks, if we let \(\eta(s)\) denote the total time spent in a state in an episode, then we can calculate \(\mu(s)\) as</p> \[\mu(s) = \frac{\eta(s)}{\sum_s \eta(s)}\] <p>In our derivation, \(P(s \rightarrow x, k, \pi )\) for very long walks can be estimated by the total time spent in the state \(s\). Thus, we can inject \(\eta(s)\) into our equation as follows:</p> \[\begin{aligned} \nabla_{\mathbf{\theta}}J(\mathbf{\theta}) &amp; = \sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}} \nabla\pi(a|s) q_\pi(s,a) \\ &amp; = \sum_{s \in \mathcal{S}} \eta(s) \sum_{s \in \mathcal{S}} \frac{\eta(s)}{\sum_{s \in \mathcal{S}} \eta(s)} \sum_{a \in \mathcal{A}} \nabla\pi(a|s) q_\pi(s,a) \\ &amp; \propto \sum_{s \in \mathcal{S}} \mu(s) \sum_{a \in \mathcal{A}} \nabla\pi(a|s) q_\pi(s,a) \end{aligned}\] <p>Thus, we get the form of the theorem as</p> \[\nabla_{\mathbf{\theta}}J(\mathbf{\theta}) \propto \sum_{s \in \mathcal{S}} \mu(s) \sum_{a \in \mathcal{A}} q_\pi(s,a) \nabla\pi(a|s)\] <p>The proportionality is the average length of an episode. In the case of continuous tasks, this is 1. Thus, now we see that we have a gradient over a parameterized policy, which allows us to move in the direction of maximizing this gradient i.e gradient ascent. We can estimate this gradient through different means.</p> <h2 id="reinforce-monte-carlo-sampling">REINFORCE: Monte-Carlo Sampling</h2> <p>For Monte-Carlo sampling of the policy, our essential requirement is sampling from a distribution that allows us to get an estimate of the policy. From the equation of the policy gradient theorem, we can write this again as an expectation over a sample of states \(S_t \sim s\) in the direction of the policy gradient</p> \[\nabla_{\mathbf{\theta}}J(\mathbf{\theta}) = \mathbb{E}_\pi \bigg [\sum_{a \in \mathcal{A}} q_\pi(S_t,a) \nabla_{\mathbf{\theta}} \pi(a|S_t, \mathbf{\theta}) \bigg ]\] <p>The expectation above would be an expectation over the actions if we were to include the probability of selecting the actions as the weight. Thus, we can do that to remove the sum over actions too:</p> \[\begin{aligned} \nabla_{\mathbf{\theta}}J(\mathbf{\theta}) &amp; = \mathbb{E}_\pi \bigg [\sum_{a \in \mathcal{A}} q_\pi(S_t,a) \nabla_{\mathbf{\theta}} \pi(a|S_t, \mathbf{\theta}) \frac{\pi(a|S_t, \mathbf{\theta})}{\pi(a|S_t, \mathbf{\theta})} \bigg ] \\ &amp; = \mathbb{E}_\pi \bigg [\sum_{a \in \mathcal{A}} \pi(a|S_t, \mathbf{\theta}) q_\pi(S_t,a) \frac{\nabla_{\mathbf{\theta}} \pi(a|S_t, \mathbf{\theta})}{\pi(a|S_t, \mathbf{\theta})} \bigg ] \\ &amp; =\mathbb{E}_\pi \bigg [ q_\pi(S_t,A_t) \frac{\nabla_{\mathbf{\theta}} \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \bigg ] \end{aligned}\] <p>The expectation over \(q_\pi (S_t, A_t)\) is essentially the return \(G_t\). Thus, we can replace that in the above equation to get:</p> \[\nabla_{\mathbf{\theta}}J(\mathbf{\theta}) = \mathbb{E}_\pi \bigg [ G_t\frac{\nabla_{\mathbf{\theta}} \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \bigg ]\] <p>We now have a full sampling of the states and actions conditioned on our parameters in the gradients. This can be considered a sample from the policy and we can update our parameters using this quantity to get our update rule as:</p> \[\begin{aligned} \mathbf{\theta}_{t+1} &amp;= \mathbf{\theta}_t + \alpha \nabla_{\mathbf{\theta}}J(\mathbf{\theta}) \\ &amp; =\mathbf{\theta}_t + \alpha \bigg ( \, G_t\frac{\nabla_{\mathbf{\theta}} \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \bigg ) \end{aligned}\] <p>This is the REINFORCE Algorithm! We have each update which is simply the learning rate \(\alpha\) multiplied by a quantity that is proportional to the return and a vector of gradients of the probability of taking a certain action in a state. From the gradient ascent logic, we can see that this vector is the direction of maximizing the probability of taking action $A_t$ again, whenever we visit $S_t$. Moreover, The update is increasing the parameter vector in this direction proportional to the return, and inversely proportional to the action probability. Since the return is evaluated till the end of the episode, this is a Monte-Carlo Algorithm. We can further refine this using the identity of log differentiation and adding the discount factor to get the update as:</p> \[\mathbf{\theta}_{t+1} = \mathbf{\theta} + \alpha \gamma^t G_t \nabla_{\mathbf{\theta}} \ln \pi(A_t| S_t , \mathbf{\theta})\] <h2 id="looking-at-trajectories">Looking at Trajectories</h2> <p>Another way to look at the above formulation is through sampled trajectories, as done in <a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf">Sergey Levine’s slides</a>. While theoretically, this treatment is similar to the one done above, I just find the notation more intuitive. Recall, a trajectory is a sequence of states and actions over time, and the rewards accumulated in this sequence qualify this trajectory. Thus, we can say that the utility objective \(J(\mathbf{\theta})\) is the sum of the accumulated rewards of some number of trajectories sampled from a policy \(\pi_\theta\)</p> \[J(\mathbf{\theta}) = \mathbb{E}_{\tau \sim \pi_{\mathbf{\theta}} (\tau)} \bigg [ \sum_t r(s_t, a_t) \bigg ] \approx \frac{1}{N} \sum_i \sum_t r(s_{i:t}, a_{i:t})\] <p>Let this sum of reward be denoted by \(G(\tau)\) for a trajectory \(\tau\). Thus, we can re-write the above equation as</p> \[J(\mathbf{\theta}) = \mathbb{E}_{\tau \sim \pi_{\mathbf{\theta}}(\tau) } \big [ G(\tau) \big ]\] <p>This expectation is essentially sampling a trajectory from a policy and weighing it with the accumulated rewards. Thus, we can write it as</p> \[J(\mathbf{\theta}) = \int \pi_{\mathbf{\theta}} (\tau) G(\tau) d\tau\] <p>Now, we just differentiate this objective, and add a convenient policy term to make it an expectation:</p> \[\begin{aligned} \nabla_{\mathbf{\theta}} J(\mathbf{\theta}) &amp; = \nabla_{\mathbf{\theta}} \int \pi_{\mathbf{\theta}} (\tau) G(\tau) d\tau \\ &amp; = \int \nabla_{\mathbf{\theta}} \pi_{\mathbf{\theta}} (\tau) G(\tau) d\tau \\ &amp; = \int \pi_{\mathbf{\theta}}(\tau) \frac{\nabla_{\mathbf{\theta}} \pi_{\mathbf{\theta}}(\tau) }{\pi_{\mathbf{\theta}}(\tau)} G(\tau) d\tau \end{aligned}\] <p>Now, we just use an identity \(\frac{dx}{x} = d \log x\) and get</p> \[\nabla_{\mathbf{\theta}} J(\mathbf{\theta}) = \int \pi_{\mathbf{\theta}} (\tau) \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}} (\tau ) G(\tau) d\tau\] <p>Thus, we can write this as:</p> \[\nabla_{\mathbf{\theta}} J(\mathbf{\theta}) = \mathbb{E}_{\tau \sim \pi_{\mathbf{\theta}} (\tau)} \big [ \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}} (\tau ) G(\tau) \big ]\] <p>Hence, we get the same final result as before: the gradient depends on the gradient of the log policy weighted by the rewards accumulated over the trajectory. Now, we can translate this to states and actions over the trajectory by simply considering what the policy of the trajectory represents:</p> \[\begin{aligned} &amp; \pi_{\mathbf{\theta}} (s_1, a_1, ..., s_T, a_T) = \mu(s) \prod_{t=1}^{T} \pi_{\mathbf{\theta}}(a_t|s_t)p(s_{t+1}|s_t, a_t) \\ \implies &amp; \log \pi_{\mathbf{\theta}} (s_1, a_1, ..., s_T, a_T) = \log \mu(s) + \sum_{t=1}^{T} \log \pi_{\mathbf{\theta}}(a_t|s_t) + \log p(s_{t+1}|s_t, a_t) \end{aligned}\] <table> <tbody> <tr> <td>When we differentiate this log policy w.r.t \(\mathbf{\theta}\), we realize that \(\mu(s)\) and $$p(s_{t+1}</td> <td>s_t, a_t)$$ do not depend on this parameter, and would be set to 0. Thus, our utility expression would end up looking like</td> </tr> </tbody> </table> \[\nabla_{\mathbf{\theta}} J(\mathbf{\theta}) = \mathbb{E}_{\tau \sim \pi_{\mathbf{\theta}} (\tau)} \big [ \sum_{t=1}^{T} \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}}(a_t|s_t) \sum_{t=1}^{T}r(s_t, a_t) \big ]\] <p>Then we take average of the samples as the expectation, we get</p> \[\nabla_{\mathbf{\theta}} J(\mathbf{\theta}) \approx \frac{1}{N} \sum _{i=1}^N \bigg [ \sum_{t=1}^{T} \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}}(a_t|s_t) \sum_{t=1}^{T}r(s_t, a_t) \bigg ]\] <p>And this is the key formula behind REINFORCE again and the update can thus, be written as</p> \[\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha \nabla_{\mathbf{\theta}} J(\mathbf{\theta})\] <p>This is where I find it more intuitive to just use \(\tau\) for trajectories since now we can just write our REINFORCE algorithm as :</p> <ol> <li> <table> <tbody> <tr> <td>Sample a set of trajectories \(\{ \tau ^i\}\) from the policy $$\pi_{\mathbf{\theta}}(a_t</td> <td>s_t)$$</td> </tr> </tbody> </table> </li> <li>Estimate \(\nabla_{\mathbf{\theta}} J(\mathbf{\theta})\)</li> <li>Update the parameters \(\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha \nabla_{\mathbf{\theta}} J(\mathbf{\theta})\)</li> </ol> <h2 id="reducing-variance">Reducing Variance</h2> <p>The idea of parameterizing the policy and working directly with the sampled trajectories has an intuitive appeal due to its clarity. However, this approach suffers from high variance. This is because when we compute the expectation over trajectories, we are essentially sampling \(N\) different trajectories and then taking the average of the accumulated rewards. If we take this sampling to be uniform, we can easily imagine scenarios, where the trajectories sampled, have wildly different accumulated rewards. Thus, the chances of getting a high variance in the values that we are averaging over increase. If we were to scale \(N\) to \(\infty\) then our average becomes closer to the true expectation. However, this is computationally expensive. There are multiple ways to reduce this variance.</p> <h3 id="rewards-to-go">Rewards to go</h3> <p>In the trajectory formulation, we are accumulating all of the rewards from \(t=0\) to \(t = N\). However, one way to make this online would be to just consider the rewards from the timestep at which we take the policy log value till the end of the horizon. This can be written as</p> \[\nabla_{\mathbf{\theta}} J(\mathbf{\theta}) \approx \frac{1}{N} \sum _{i=1}^N \bigg [ \sum_{t=1}^{T} \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}}(a_t|s_t) \sum_{t'=t}^{T}r(s_{t'}, a_{t'}) \bigg ]\] <p>Thus, by reducing the number of rewards we consider for each policy evaluation, we are essentially better able to control the variance up to a certain extent</p> <h3 id="baselining">Baselining</h3> <p>Another way to control the variance is to realize that the actions in a state are the quantities that create a variance for each state in trajectory. However, we see that our return \(G(\tau)\) is dependent on both states and actions. Thus, if we could compare this value to a baseline value \(b(s)\), we eliminate the variance resulting from fixed state selection. In the policy gradient theorem, this would look like</p> \[\nabla_{\mathbf{\theta}}J(\mathbf{\theta}) \propto \sum_{s \in \mathcal{S}} \mu(s) \sum_{a \in \mathcal{A}} \big ( q_\pi(s,a) - b(s) \big ) \nabla\pi(a|s)\] <p>since \(b(s)\) does not vary with actiosn, it would not have an effect on our summation since its sum over all actions would be 0:</p> \[\begin{aligned} \sum_a b(s) \nabla_{\mathbf{\theta}}\pi(a|s, \mathbf{\theta}) &amp; = b(s) \nabla_{\mathbf{\theta}} \sum_a \pi(a|s, \mathbf{\theta}) \\ &amp; = b(s) \nabla_{\mathbf{\theta}} 1 \\ &amp; = 0 \end{aligned}\] <p>Thus, we can effectively update our REINFORCE update with this baseline to get</p> \[\mathbf{\theta}_{t+1} = =\mathbf{\theta}_t + \alpha \big ( \, G_t - b(s) \big ) \frac{\nabla_{\mathbf{\theta}} \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})}\] <p>Or, in the trajectory formulation</p> \[\mathbf{\theta}_{t+1} = =\mathbf{\theta}_t + \alpha \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}} (\tau ) \big (G(\tau) - b(s) \big )\] <p>One good function for \(b(s)\) could be the estimate of the state value \(\hat{v}(s_t, \mathbf{w})\) . Doing something like this may seem like going to the realm of actor-critic methods, where we are parameterizing the policy and using the value function, but this is not the case here since we are not using the value function to bootstrap. We are stabilizing the variance by using the estimate of the value function as a baseline. Baselines are not just limited to this. We can inject all kinds of things into the baseline to try scaling up our policy gradient. For example, an <a href="https://arxiv.org/abs/2102.10362">interesting paper</a> published recently talks about using functions that take into account causal dependency as a Baseline. There are many other extensions like Deterministic Policy Gradients, Deep Deterministic Policy Gradients, Proximal Optimization e.t.c that look deeper into this problem.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[The core idea of Reinforcement learning is to learn some kind of behavior through optimizing for rewards. The behavior learned by an agent i.e. the schema it follows while going through this process is the learned policy that it uses to decide which action to take and thus, the transition from one state to another. One way to close the loop for the agent to learn is by evaluating the states and actions through value functions and thus, our way to measure the learned policy is seen through these value functions, approximated by lookup tables, linear combinations, Neural Networks e.t.c. Policy Gradient methods take a different approach where they bypass the need for a value function by parameterizing the policy directly. While the agent can still use a value function to learn, it need not use it for selecting actions. The advantages that policy gradient methods offer are 3 fold:]]></summary></entry><entry><title type="html">Model-Free Prediction</title><link href="https://amsks.github.io/blog/2023/model-free-prediction/" rel="alternate" type="text/html" title="Model-Free Prediction"/><published>2023-07-10T12:57:00+00:00</published><updated>2023-07-10T12:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/model-free-prediction</id><content type="html" xml:base="https://amsks.github.io/blog/2023/model-free-prediction/"><![CDATA[<p>One of the problems with DP is that it assumes full knowledge of the MDP, and consequently, the environment. While this holds true for a lot of applications, it might not hold true for all cases. In fact, the upper limit does turn out to be the ability to be accurate about the underlying MDP. Thus, if we don’t know the true MDP behind a process, the next best thing would be to try to approximate them. One of the ways to go about this is <strong>Model-Free RL</strong>.</p> <h2 id="monte-carlo-methods">Monte-Carlo Methods</h2> <p>The core idea behind the Monte-Carlo approach, which has its root in gambling, is to use probability to approximate quantities. Suppose we have to approximate the area of a circle relative to a rectangle inside which it is inscribed (This is a classic example and an easy experiment), the experiment-based approach would be to make a board and randomly throw some balls on it. In a true random throw, let’s call the creation of a spot on the board a simulation ( experiment, whatever!). After each simulation, we record the number of spots inside the circular area and the total number of spots including the circular area and the rectangular area. A ratio of these two quantities would give us an estimate of the relative area of the circle and the rectangle. Now as we conduct more such experiments, this estimate would actually get better since the underlying probability of a spot appearing inside the circle is proportional to the amount of area that the circle occupies inside the rectangle. Hence, if we keep doing this our approximation gets increasingly closer to the true value.</p> <h3 id="applying-mc-idea-to-rl">Applying MC idea to RL</h3> <p>Another way to put the Monte-Carlo approach would be to say that Monte-Carlo methods only require experience. In the case of RL, this would translate to sampling sequences of states, actions, and rewards from actual or simulated interaction with an environment. An experiment in this sense would be a full rollout of an episode which will create a sequence of states and rewards. When multiple such experiments are conducted, we get better approximations of our MDP. To be specific, our goal here would be to learn \(v_{\pi}\) from the episodes under policy \(\pi\). The value function is the expected reward:</p> \[v_{\pi}(s)= \mathbb{E}_{\pi}[G_t∣S_t=s]\] <p>So, all we have to do is estimate this expectation using the empirical mean of the returns for the experiments</p> <h3 id="first-visit-mc-evaluation">First-Visit MC Evaluation</h3> <p>To evaluate state s, at the first time-step t at which s is visited:</p> <ol> <li>Increment counter:  \(N(s) \leftarrow N(s) + 1\)</li> <li>Increment total return: \(S(s) \leftarrow S(s) + 1\)</li> <li>Estimate value by the mean return: \(V(s) = \frac{S(s)}{N(s)}\) </li> </ol> <p>As we repeat more experiments and update the values at the first visit, we get convergence to optimal values i.e \(V(s) \rightarrow v_{\pi}\) as \(N(s) \rightarrow \infty\)</p> <h3 id="every-visit-mc-evaluation">Every-Visit MC Evaluation</h3> <p>This is same as first visit evaluation, except we update at every visit:</p> <ol> <li> \[N(s) \gets N(s) + 1\] </li> <li> \[S(s) \gets S(s) + 1\] </li> <li> \[V(s) = \frac{S(s)}{N(s)}\] </li> </ol> <h3 id="incremental-mc-updates">Incremental MC updates</h3> <p>The empirical mean can be expressed as an incremental update as follows:</p> \[\begin{aligned} \mu_k &amp;= \frac{1}{k} \sum_{j=1}^n x_j \\ &amp;= \frac{1}{k} (x_k + \sum_{j=1}^{k-1} x_j) \\ &amp;= \frac{1}{k} (x_k + (k-1)\mu_{k-1}) \end{aligned}\] <p>Thus, for the state updates, we can follow a similar pattern, and for each state \(S_t\) and return \(G_t\), express it as:</p> <ol> <li> \[N(S_t) \gets N(S_t) + 1\] </li> <li> \[(S_t) = V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))\] </li> </ol> <p>Another useful way would be to track a running mean:</p> \[V(S_t) \gets V(S_t) + \alpha (G_t - V(S_t)\] <h2 id="temporal-difference-td-learning">Temporal-Difference (TD) Learning</h2> <p>The MC method of learning needs an episode to terminate in order to work its way backward. In TD, the idea is to work the way forward by replacing the remainder of the states with an estimate. This is one method that is considered central and novel to RL (According to Sutton and Barto). Like MC methods, TD methods can learn directly from raw experience, and like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome, something which is called Bootstrapping - updating a guess towards a guess (meta-guess-update?).</p> <h3 id="concept-of-target-and-error">Concept of Target and Error</h3> <p>If we look at the previous equation of Incremental MC, the general form that can be extrapolated is</p> \[V(S_t) \gets V(S_t) + \alpha (T - V(S_t)\] <p>Here, the quantity \(T\) is called the <strong>Target</strong> and the quantity \(T - V(S_t)\) is called the <strong>error</strong>. In the MC version, the target is the return \(G_t\), which means that the MC method has to wait for this return to be propagated backward to see the error of its current value function from this return, and improve. This is where TD methods show their magic; At time \(t+1\), they can immediately form a target and make a useful update using the observed reward and the current estimate of the value function. The simplest TD method, \(TD(0)\), thus has the following form:</p> \[V(S_t) \gets V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))\] <p>This is why bootstrapping is a guess of guess, since the TD method bases its update in part on an existing estimate.</p> <h2 id="comparing-td-mc-and-dp">Comparing TD, MC and DP</h2> <p>Another way to look at these algorithms would be through the Bellman optimality equation:</p> \[v_{\pi}(s) = E [ R_{t+1} + \gamma v_{\pi}(S_{t+1})| S_t = s]\] <p>which allows us to see why certain methods are estimates:</p> <ul> <li>The MC method is an estimate because it does not have a model of the environment and thus, needs to sample in order to get an estimate of the mean.</li> <li>The DP method is an estimate because it does not know the future values of states and thus, uses the current state value estimate in its place.</li> <li>The TD method is an estimate because it does both of these things. Hence, it is a combination of both. However, unlike DP, MC and TD do not require a model of the environment. Moreover, the online nature of these algorithms is something that allows them to work with samples of backups, whereas DP requires full backup.</li> </ul> <p>TD and MC can further be differentiated based on the nature of the samples that they work with:</p> <ul> <li>TD requires shallow backups since it is inherently online in nature</li> <li>MC requires deep backups due to the nature of its search.</li> </ul> <p>Another way to look at the inherent difference is to realize that DP inherently does a breadth-first search, while MC does a depth-first search. TD(0) only looks one step ahead and forms a guess. These differences can be summarized on the following spectrum by David Silver, which I find really helpful:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/TD-MC-DP-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/TD-MC-DP-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/TD-MC-DP-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/TD-MC-DP.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="extending-td-to-n-steps">Extending TD to n-steps</h2> <p>The next natural step for something like TD would be to extend it to further steps. For this, we generalize the target and define it as follows:</p> \[G^{(n)}_t = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+ n} + \gamma^n V(S_{t+n}))\] <p>And so, the equation again follows the same format:</p> \[V(S_t) \gets V(S_t) + \alpha (G^{(n)}_t - V(S_t)\] <p>One interesting thing to note here is that if the value of \(n\) is increased all the way to the terminal state, then we essentially get the same equation as MC methods!</p> <h3 id="averaging-over-n-returns">Averaging over n returns</h3> <p>To get the best out of all the \(n\) steps, one improvement could be to average the returns over a certain number of states. For example, we could combine 2-step and 4-step returns and take the average :</p> \[G_{avg} = \frac{1}{2} [ G^{(2)} + G^{(4)} ]\] <p>This has been shown to work better in many cases, but only incrementally.</p> <h3 id="lambda-return--forward-view">\(\lambda\)-Return → Forward View</h3> <p>This is a method to combine the returns from all the n-steps:</p> \[G^{(\lambda)}_t = (1 - \lambda ) \displaystyle\sum_{n=1}^{\infin} \lambda^{n-1} G^{(n)}_t\] <p>And this, is also called <strong>Forward-view \(TD(\lambda)\).</strong></p> <h3 id="backward-view">Backward View</h3> <p>To understand the backward view, we need a way to see how we are going to judge the causal relationships between events and outcomes (Returns). There are two heuristics:</p> <ol> <li><strong>Frequency Heuristic →</strong> Assign credit to the most frequent states</li> <li><strong>Recency Heuristic →</strong> Assign credit to the most recent states.</li> </ol> <p>The way we keep track of how each states fares on these two heuristics is through <strong>Eligibility Traces</strong>:</p> \[E_t(s) = \gamma \lambda E_{t-1}(s) + \bold{1}(S_t = s)\] <p>These traces accumulate as the frequency increases and are higher for more recent states. If the frequency drops, they also drop. This is evident in the figure below:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/ET-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/ET-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/ET-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/ET.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>So, all we need to do it scale the TD-error \(\delta_t\) according to the trace function:</p> \[V(S) \gets V(S) + \alpha \delta_t E_t(s)\] <p>Thus, when \(\lambda = 0\), we get the equation for \(TD(0)\) , and when \(\lambda =1\), the credit is deferred to the end of the episode and we get MC equation.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[One of the problems with DP is that it assumes full knowledge of the MDP, and consequently, the environment. While this holds true for a lot of applications, it might not hold true for all cases. In fact, the upper limit does turn out to be the ability to be accurate about the underlying MDP. Thus, if we don’t know the true MDP behind a process, the next best thing would be to try to approximate them. One of the ways to go about this is Model-Free RL.]]></summary></entry><entry><title type="html">Introduction to Reinforcement learning</title><link href="https://amsks.github.io/blog/2023/introduction-to-reinforcement-learning/" rel="alternate" type="text/html" title="Introduction to Reinforcement learning"/><published>2023-07-09T12:57:00+00:00</published><updated>2023-07-09T12:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/introduction-to-reinforcement-learning</id><content type="html" xml:base="https://amsks.github.io/blog/2023/introduction-to-reinforcement-learning/"><![CDATA[<p>One way to look at the behavior of an organism is by looking at how it interacts with its environment, and how this interaction allows it to behave differently over time through the process of learning. In this view, the behavior of the organism can be modeled in a closed-loop manner through a fundamental description of the action and sensation loop. It receives input - sensation - from the environment through sensors, acts on the environment through actuators, and observes the impact of its action on its understanding of the environment through a mechanism of quantification in the form of the rewards it receives for its action. A similar thing happens in RL. The thing that we need to train is called an Agent. The language of communication with this agent is through numbers, encoded in processes that we create for it to understand and interact with the world around it. The way this agent interacts with the world around it is through Actions (A) and the way it understands the world is through Observations (O). Now, our task is to define these actions and observations and train this agent to achieve a certain task by creating a closed-loop control of feedback for the actions it takes. This feedback is the Reward (R) that agent receives for each of its actions. So, the key is to devise a method to guide the agent in such a way that it ‘learns’ to reach the goal by selecting actions with the highest Expected Rewards (G), updating these values by observing the environment after taking that action. Thus, the agent first takes random actions and updates its reward values, and slowly, it starts to favor actions with higher rewards, which eventually lead to the goal.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/agent-env.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/agent-env.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/agent-env.svg-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/agent-env.svg" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The way we define observations is through formalizing it as a <strong>State (S)</strong> in which this agent exists, or can exist. This state can either be the same as the observation, in case the agent can see everything about its environment, for example, in an extreme case imagine if you were able to see all the atoms that constitute your surroundings, or the state can be defined in terms of <strong>Beliefs (b)</strong> that agent the might have based on its observation. this distinction is important for problems of partial observability, a topic for the future. A standard testbed in RL is the Mountain Car scenario. As shown in the figure below, the car exists in a valley and the goal is at the top. The car needs to reach this goal by accelerating, but it is unable to reach the top by simply accelerating from the bottom. Thus, it must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/mountain-car-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/mountain-car-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/mountain-car-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/mountain-car.jpg" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>One way to define the values for the agent - the car - would be to define the state as the (position, velocity) of the car, the actions as (Do nothing, Push the car left, Push the car right), and rewards as -1 for each step that leads to a position that is not the goal and 0 for reaching the goal. To characterize the agent, the following components are used in the RL vocabulary:</p> <ul> <li><strong>Policy (\(\pi: S \rightarrow A\)):</strong> This is the behavior of the agent that i.e the schema it follows while navigating in the environment it observes by taking actions. Thus, it is a mapping from state to action</li> <li><strong>Value Function (V):</strong> This is the agent’s prediction of future rewards. The way this fits into the picture is that at each step the agent predicts the rewards that it can get in the future by following a certain set of actions under a policy. This expectation of reward is what determines which actions the agent should select.</li> <li><strong>Model:</strong> The agent might make a model of the world that it observes around itself. Then it can use this model to extract information that it can use to better decide the actions that it can take. There are two types of models that are used, Reward Model and Transition</li> <li><strong>Reward Model:</strong> Model to predict the next immediate reward. This is defined in terms of Expectation fo reward conditioned on a state and action :</li> </ul> \[R^{a}_{s} = \mathbb{E}[ R | S=s, A=a ]\] <ul> <li> <p><strong>Transition Model:</strong> Model to predict the next state using the dynamics of the environment. This is defined in terms of probability of a next state, conditioned on the current state and actions :</p> \[P^{a}_{ss'} = \mathbb{P}[ S'=s'| S=s, A=a ]\] </li> </ul> <p>Thus, using the above components learning can be classified into three kinds:</p> <ol> <li><strong>Value-Based RL:</strong> In this type, the agent uses a value function to track the quality of states and thus, follows trends in the value functions. For example, in a maze with discretized boxes as steps, the agent might assign values to each step and keep updating them as it learns, and thus, end up creating a pattern where a trend of following an increase in the value would inevitably lead to the way out of the maze</li> <li><strong>Policy-Based RL:</strong> In this case, the agent would directly work with the policy. So, in the case of the maze example, each step might be characterized by four directions in which the agent can traverse (up, down, left, right) and for each box, the agent might assign a direction it will follow once it reaches that, and as it learns it can update these directions t create a clear path to the end of the maze</li> <li><strong>Actor-Critic:</strong> If two ideas are well-established in the scientific community, in this case, the value-based, and policy-based approach, then the next best step could be to try and merge them to get the best of both worlds. This is what the actor-critic does; it tries to merge both these ideas by splitting the model into two parts. The actor takes the state as an input and outputs the best actions by following a learned optimal policy (policy-based learning). The critic generates the value for this action by evaluating the value function ( value-based learning). These both compete in a game to improve their methods and overall the agent learns to perform better.</li> </ol> <p>The learning can also be distinguished based on whether the agent has a model of the world, in which case the learning is <strong>Model-Based RL</strong>, or whether the agent operates without a model of the world i.e <strong>Model-Free RL</strong>. This will be explored in more detail in the next sections. Finally, certain paradigms are common in RL which recurs regularly, and thus, it might be good to list them down:</p> <ul> <li><strong>Learning and Planning:</strong> In learning the rules of the game are unknown and are learned by putting the agent in the environment. For example, I remember some people once told me how some coaches teach the basics of swimming by asking the learner to directly jump into the semi-deep water and try to move their hands and legs in a way so that they can float. Irrespective of whether this actually happens or not, if someone learned this way I could think of it as a decent enough analogy. Planning, on the other hand, is driven by a model of the rules that need to be followed, which can be used by the agent to perform a look-ahead search on the actions that it can take.</li> <li><strong>Exploration and Exploitation:</strong> This is the central choice the agent needs to make every time it takes an action. At any step, it has certain information about the world and it can go on exploiting it to eventually reach a goal (maybe), but the problem is it might not know about the most optimal way to reach this goal if it just acts on the information it already has. Thus, to discover better ways of doing things, the agent can also decide to forego the path it ‘knows’ will get the best reward according to its current knowledge and take a random action to see what kind of reward it gets. Thus, in doing so the agent might end up exploring other ways of solving a problem that it might not have known, which might lead to higher rewards than the path it already knows. Personally, the most tangible way I can visualize it is by thinking of a tree of decisions, and then imagining that the agent knows one way to reach the leaf nodes with the maximum reward. However, there might exist another portion of the tree that has higher rewards, but the agent might not ever go to if it greedily acts on its current rewards.</li> <li><strong>Prediction and Control:</strong> Prediction is just finding a path to the goal, while control is optimizing this path to the goal. Most of the algorithms in RL can be distinguished based on this.</li> </ul>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[One way to look at the behavior of an organism is by looking at how it interacts with its environment, and how this interaction allows it to behave differently over time through the process of learning. In this view, the behavior of the organism can be modeled in a closed-loop manner through a fundamental description of the action and sensation loop. It receives input - sensation - from the environment through sensors, acts on the environment through actuators, and observes the impact of its action on its understanding of the environment through a mechanism of quantification in the form of the rewards it receives for its action. A similar thing happens in RL. The thing that we need to train is called an Agent. The language of communication with this agent is through numbers, encoded in processes that we create for it to understand and interact with the world around it. The way this agent interacts with the world around it is through Actions (A) and the way it understands the world is through Observations (O). Now, our task is to define these actions and observations and train this agent to achieve a certain task by creating a closed-loop control of feedback for the actions it takes. This feedback is the Reward (R) that agent receives for each of its actions. So, the key is to devise a method to guide the agent in such a way that it ‘learns’ to reach the goal by selecting actions with the highest Expected Rewards (G), updating these values by observing the environment after taking that action. Thus, the agent first takes random actions and updates its reward values, and slowly, it starts to favor actions with higher rewards, which eventually lead to the goal.]]></summary></entry><entry><title type="html">Markov Processes</title><link href="https://amsks.github.io/blog/2023/markov-processes/" rel="alternate" type="text/html" title="Markov Processes"/><published>2023-07-09T12:57:00+00:00</published><updated>2023-07-09T12:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/markov-processes</id><content type="html" xml:base="https://amsks.github.io/blog/2023/markov-processes/"><![CDATA[<p>These are random processes indexed by time and are used to model systems that have limited memory of the past. The fundamental intuition behind Markov processes is the property that the future is independent of the past, given the present. In a general scenario, we might say that to determine the state of an agent at any time instant, we only have to condition it on a limited number of previous states, and not the whole history of its states or actions. The size of this window determines the order of the Markov process.</p> <p>To better explain this, one primary point that needs to be addressed is that the complexity of a Markov process greatly depends on whether the time axis is discrete or topological. When this space is discrete, then the Markov process is a Markov Chain. A basic level understanding of how these processes play out in the domain of reinforcement learning is very clear when analyzing these chains. Moreover, the starting point of analysis can be further simplified by limiting the order of Markov Processes to first-order. This means that at any time instant, the agent only needs to see its previous state to determine its current state, or its current state to determine its future state. This is called the <strong>Markov Property</strong></p> \[\mathbb{P}(S_{t+1}|S_t) = \mathbb{P}(S_{t+1}|S_1, ..., S_t) )\] <h2 id="markov-process">Markov Process</h2> <p>The simplest process is a tuple \(&lt;S,P&gt;\) of states and Transitions. The transitions can be represented as a Matrix \(P = [P_{ij}]\), mapping the states - \(i\) - from which the transition originates, to the states - \(j\) - to which the transition goes.</p> \[\begin{bmatrix} P_{11} &amp; . &amp; . &amp; . &amp; P_{1n}\\ . &amp; . &amp; . &amp; . &amp; . \\ . &amp; . &amp; . &amp; . &amp; . \\ . &amp; . &amp; . &amp; . &amp; . \\ P_{n1} &amp; . &amp; . &amp; . &amp; P_{nn} \end{bmatrix}\] <p>Another way to visualize this would be in the form of a graph, as shown below, courtesy of David Silver.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/MP-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/MP-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/MP-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/MP.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This is a basic chain that represents the actions a student can take in the class, with associated probabilities of taking those actions. Thus, in the state - Class 1 - the student has an equal chance of going to the next class or browsing Facebook. Once they start browsing Facebook, then they have a 90% chance of continuing to browse since it is addictive. Similarly, other states can be seen too.</p> <h2 id="markov-reward-process">Markov Reward Process</h2> <p>Now if we add another parameter of rewards to the Markov processes, then the scenario changes to the one in which entering each state has an associated expected immediate reward. This, now, becomes a Markov Reward Process.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/MRP-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/MRP-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/MRP-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/MRP.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>To fully formalize this, one more thing that needs to be added is the discounting factor \(\gamma\). This is a hyperparameter that represents the amount of importance we give to future rewards, something like a ‘shadow of the future’. The use of Gamma can be seen in computing the return \(G_t\) on a state:</p> \[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...\] <p>The reasons for adding this discounting are:</p> <ul> <li>To account for uncertainty in the future, and thus, better balance our current decisions → The larger the value, the more weightage we give to the ‘shadow of the future’</li> <li>To make the math more convenient → only when we discount the successive terms, we can get convergence on an infinite GP</li> <li>To avoid Infinite returns, which might be possible in loops within the reward chain</li> <li>This is similar to how biological systems behave, and so in a certain sense, we are emulating nature.</li> </ul> <p>Thus, the reward process can now be characterized by the tuple \(&lt;S, P, R, \gamma &gt;\) . To better analyze the Markov chain, we will also define a way to estimate the value of a state - <strong>Value function</strong> - as an expectation of the Return on that state. Thus,</p> \[V(S) = \mathbb{E} [ G_t| S_t = s ]\] <p>An intuitive way to think about this is in terms of betting. Each state is basically a bet that our agent needs to make. Thus, the process of accumulating the rewards represents the agent’s understanding of each of these bets, and to qualify them, the agent has to think in terms of the potential returns that these bets can give. This is what we qualify here as the expectation. But the magic comes when we apply it recursively, and this is called the <strong>Bellman Equation</strong></p> \[V(S) = \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}| S_t = s]\] <p>This equation signifies that the value of the current state can be seen in terms of the value of the next state and so on, and thus, we can have a correlated relationship between states. To better see how this translates to the whole chain, we can also express this as a Matrix Operation:</p> \[\begin{bmatrix} V_1 \\ . \\ . \\ V_n \end{bmatrix} = \begin{bmatrix} R_1 \\ . \\ . \\ R_n \end{bmatrix} \begin{bmatrix} P_{11} &amp; . &amp; . &amp; P_{1n}\\ . &amp; . &amp; . &amp; . \\ . &amp; . &amp; . &amp; . \\ P_{n1} &amp; . &amp; . &amp; P_{nn} \end{bmatrix} \begin{bmatrix} V_1 \\ . \\ . \\ V_n \end{bmatrix}\] <p>And so, the numerical way to solve this would be to invert the matrix (assuming it is invertible) and the solution, then would be:</p> \[\bm{\bar{V}} = (1 - \gamma \bm{\bar{P}})^{-1} \bm{\bar{R}}\] <p>However, as anyone familiar with large dimensions knows, this becomes intractable pretty easily. Hence, the whole of RL is based on figuring out ways to make this tractable, using majorly three kinds of methods:</p> <ol> <li>Dynamic Programming</li> <li>Monte-Carlo Methods</li> <li>Temporal Difference Learning</li> </ol> <h2 id="markov-decision-process">Markov Decision Process</h2> <p>If we add actions to the Markov Reward Process, then there can multiple states that the agent can reach by taking action to each state. Thus, the agent now has to decide which action to take. This is called a Markov Decision Process.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/MDP-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/MDP-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/MDP-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/MDP.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Thus, the MDP can be summarized by the tuple \(&lt;S, A, P, R, \gamma &gt;\). Here, we can also define the transitions and rewards in terms of the actions:</p> \[\begin{aligned} R^{a}_s &amp;= \mathbb{E}[ R{t+1} | S_t=s, A_t=a ] \\ P^{a}_{ss'} &amp;= \mathbb{P}[ S{t+1}=s'| S_t=s, A_t=a ] \end{aligned}\] <p>Now, the important thing is how the agent makes these decisions. The schema that the agent follows for this is called a <strong>Policy</strong>, which can be seen as the probability of taking an action, given the state:</p> \[\pi (a|s) = \mathbb{P} [ A_t = a | S_t = s ]\] <p>Under a particular policy \(\pi\), the Markov chain that results is nothing but an MRP, since we don’t consider the actions that the agent did not take. This can be characterized by \(&lt;S, P^{\pi}, R^{\pi}, \gamma&gt;\), and the respective transitions and rewards can be described as:</p> \[\begin{aligned} R^{\pi}_{s} &amp;= \sum_{\substack{a \in A}} \pi (a|s) R^{a}_{s}\\ P^{\pi}_{ss'} &amp;= \sum_{a \in A} \pi (a|s) P^{a}_{ss'} \end{aligned}\] <p>Another important thing that needs to be distinguished here is the value function, which can be defined for both states and actions:</p> <ul> <li> <p><strong>State-Value Function (\(v_{\pi}\)):</strong> Values for states when policy \(\pi\) is followed</p> \[v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]\] </li> <li> <p><strong>Action-Value Function (\(q_{\pi}\)):</strong> Expected return on starting from state \(s\), following policy \(\pi\), and taking action \(a\)</p> \[q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]\] </li> </ul> <h2 id="bellman-equation-for-mdps">Bellman Equation for MDPs</h2> <p>We can extend the bellman formulation to recursively define the qualities of state and actions:</p> \[\begin{aligned} &amp;v_{\pi}(s) = \mathbb{E}_{\pi} \big[R_{t+1} + \gamma v_{\pi}(s')| S_t = s, S_{t+1} = s'\big] \\ &amp;q_{\pi}(s, a) = \mathbb{E}_{\pi} \big[R_{t+1} + \gamma q_{\pi}(s', a')| S_t = s, S_{t+1} = s', A_t = a, A_{t+1} = a' \big] \end{aligned}\] <p>However, a better way is to look at the inter-dependencies of these two value functions. The value of the state can be viewed as the sum of the value of the actions that can be taken from this state, which can, in turn, be viewed as the weighted sum of values of the states that can result from each action.</p> <p>The expectation for the value of the states is the sum of the values of the actions that can result from that state</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/sve-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/sve-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/sve-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/sve.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Thus, under the policy \(\pi\) this value is the sum of the q-values of the actions:</p> \[v_{\pi}(s) = \sum_{a \in A} \pi (a|s) q_{\pi} (s,a)\] <p>Now, the action can be viewed in a similar manner as a sum over the value fo the states that can result from it</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/ave-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/ave-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/ave-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/ave.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>and written in the same manner</p> \[q_{\pi}(s,a) = R^{a}_s + \gamma \sum_{s' \in S} P^{a}_{ss'} v_{\pi} (s)\] <p>And, if we put these equations together, we can get a self-recursive formulation of the bellman expectation equation. Thus, for the state this would be</p> \[v_{\pi}(s) = \sum_{a \in A} \pi (a|s) [ R^{a}_s + \gamma \sum_{s' \in S} P^{a}_{ss'} v_{\pi} (s) ]\] <p>A Visualization for this would basically be a combination of the above two trees</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/sveave-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/sveave-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/sveave-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/sveave.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>A similar process can be done for the action value function, and the result comes out to be</p> \[q_{\pi}(s,a) = R^{a}_s + \gamma \sum_{s' \in S} P^{a}_{ss'} \sum_{a' \in A} \pi (a'|s') q_{\pi} (s',a')\] <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/avesve-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/avesve-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/avesve-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/avesve.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="bellman-optimality-equation">Bellman Optimality Equation</h3> <p>With the recursive forms, the question really comes on how do we go about creating a closed-loop optimality criterion. Here, the key point that needs to be taken into account is <strong>The agent is free to choose the action that it can take in each state, but it can’t choose the state that results from that action</strong>. This means, we start from a state, and maximize the result by choosing the action with the maximum action value. This is the first step of lookahead. Now, each of those actions has the associated action value that needs to be determined. In the case where the action can only lead to one state, it’s all well and good. However, in the case where multiple states can result out of the action, the value of the action can be determined by basically rolling a dice and seeing which state the action leads to. Thus, the value of the state that the action leads to determines the value of the action. This happens for all the possible actions from our first state, and thus, the value of the state is determined. Hence, with this <strong>Two-step lookahead</strong>, we can formulate the decision as maximizing the action values.</p> \[v_{\pi}(s) = \max_{a} \{ R^{a}s + \gamma \sum_{s' \in S} P^{a}_{ss'} v{} (s) \}\] <p>Now, the question arises as to how can this equation be solved. The thing to note here is the fact that it is not linear. Thus, in general, there exists no closed-form solution. However, a lot of work has been done in developing iterative solutions to this, and the primary methods are:</p> <ul> <li><strong>Value Iteration:</strong> Here methods solve the equation by iterating on the value function, going through episodes, and recursively working backward on value updates</li> <li><strong>Policy Iteration:</strong> Here, the big idea is that the agent randomly selects a policy and finds a value function corresponding to it. Then it finds a new and improved policy based on the previous value function, and so on.</li> <li><strong>Q Learning:</strong> This is a model-free way in which the agent is guided through the quality of actions that it takes, wit the aim of selecting the best ones</li> <li><strong>SARSA:</strong> Here the idea is to iteratively try to close the loop by selecting a <strong>S</strong>tate, <strong>A</strong>ction, and <strong>R</strong>eward and then seeing the <strong>S</strong>tate and <strong>A</strong>ction that follows.</li> </ul> <h2 id="extensions-to-mdp">Extensions to MDP</h2> <p>MDPS, as a concept, has been extended to make them applicable to multiple other kinds of problems that could be tackled. Some of these extensions are:</p> <ol> <li><strong>Infinite and Continuous MDPs:</strong> In this extension, the MDP concept is applied to infinite sets, mainly countably infinite state or action spaces, Continuous Spaces (LQR), continuous-time et. al</li> <li><strong>Partially Observable MDPs (POMDP):</strong> A lot of scenarios exist where there are limits on the agent’s ability to fully observe the world. These are called Partially-Observable cases. Here, the state is formalized in terms of the belief distribution over the possible observations and encoded through the history of the states. The computations become intractable in theory, but many interesting methods have been devised to get them working. Eg. DESPOT</li> <li><strong>Undiscounted and Average Reward MDP:</strong> These are used to tackle ergodic MDPs - where there is a possibility that each state can be visited an infinite number of times ( Recurrence), or there is no particular pattern in which the agent visits the states (Aperiodicity) - and to tackle this, the rewards are looked at as moving averages that can be worked with on instants of time.</li> </ol>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[These are random processes indexed by time and are used to model systems that have limited memory of the past. The fundamental intuition behind Markov processes is the property that the future is independent of the past, given the present. In a general scenario, we might say that to determine the state of an agent at any time instant, we only have to condition it on a limited number of previous states, and not the whole history of its states or actions. The size of this window determines the order of the Markov process.]]></summary></entry><entry><title type="html">Planning and Dynamic Programming</title><link href="https://amsks.github.io/blog/2023/planning-and-dynamic-programming/" rel="alternate" type="text/html" title="Planning and Dynamic Programming"/><published>2023-07-09T12:57:00+00:00</published><updated>2023-07-09T12:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/planning-and-dynamic-programming</id><content type="html" xml:base="https://amsks.github.io/blog/2023/planning-and-dynamic-programming/"><![CDATA[<p>Dynamic programming (DP) is a method that solves a problem by breaking it down into sub-problems and then solving each sub-problem individually, after which it combining them into a solution. A good example is the standard Fibonacci sequence calculation problem, where traditionally the way to solve it would be through recursion</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">fib</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="p">)</span> <span class="p">{</span>
	
	<span class="k">if</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="nf">fib</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">fib</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span>	

<span class="p">}</span>
</code></pre></div></div> <p>However, the way DP would go about this would be to cache the variables after the first call, so that the same call is not made again, making the program more efficient:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">fib</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="p">)</span> <span class="p">{</span>

	<span class="k">static</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">cache</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>
	<span class="kt">int</span><span class="o">&amp;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="n">x</span><span class="p">];</span>
	
	<span class="k">if</span> <span class="p">(</span><span class="n">result</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
	
		<span class="k">if</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="n">result</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	
		<span class="k">else</span> <span class="n">result</span> <span class="o">=</span> <span class="n">fib</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">fib</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span>
	<span class="p">}</span>
	
	<span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>The 2 characteristics that a problems need to have fo DP to solve it are:</p> <ol> <li><strong>Optimal Substructure :</strong> Any problem has optimal substructure property if its overall optimal solution can be constructed from the optimal solutions of its subproblems i.e the property \(F(n) = F(n-1) + F(n-2)\) in fibonacci numbers</li> <li><strong>Overlapping Sub-problems:</strong> The problem involves sub-problems that need to be solved recursively many times</li> </ol> <p>Now, in the case of an MDP, we have already seen that these properties are fulfilled:</p> <ol> <li>The Bellman equation gives a recursive relation that satisfies the overlapping sub-problems requirement</li> <li>The value function is able to store and re-use the solutions from each state-visit, and thus, we can exploit it as an optimal substructure</li> </ol> <p>Hence, DP can be used for making solutions to MDPs more tractable, and thus, is a good tool to solve the planning problem in an MDP. The planning problem, as discussed before, is of two types:</p> <ol> <li><strong>Prediction Problem:</strong> <strong>How do we evaluate a policy ?</strong> or, Using the MDP tuple as an input, the output is a value function \(v_\pi\) and/or a policy \(\pi\)</li> <li><strong>Control Problem:</strong> <strong>How do we optimize the policy ?</strong> Using the MDP tuple as an input, the output is an optimal value function \(v_*\) and/or a policy \(\pi_*\)</li> </ol> <h2 id="iterative-policy-evaluation">Iterative Policy Evaluation</h2> <p>The most basic way is to iteratively apply the Bellman equation, using the old values to calculate a new estimate, and then using this new estimate to calculate new values. In the Bellman equation for the state-value function</p> \[v_\pi(s) = \sum_{a \in A} \pi (a|s) \big[ R^{a}s + \gamma \sum{s' \in S} P^{a}{ss'} v_\pi(s) \big]\] <p>As long as either \(\gamma &lt; 1\) or the eventual termination is guaranteed from all states under the policy \(\pi\), the uniqueness of the value function is guaranteed. Thus, we can consider a sequence of approximation functions \(v_0, v_1, v_2, ...\) each mapping states to Real numbers, start with an arbitrary estimate of $v_0$, and obtain successive approximations using Bellman equation, as follows:</p> \[v_{k+1}(s) = \sum_{a \in A} \pi (a|s) \big[ R^{a}_s + \gamma \sum_{s' \in S} P^{a}_{ss'} v_{k} (s') \big]\] <p>The sequence \(v_k\) can be shown to converge as \(k \rightarrow \infty\). The process is basically a propagation towards the root of the decision tree from the roots.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Reinforcement-Learning/It-pol-eval-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Reinforcement-Learning/It-pol-eval-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Reinforcement-Learning/It-pol-eval-1400.webp"/> <img src="/assets/img/Reinforcement-Learning/It-pol-eval.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This update operation is applied to each state in the MDP at each step, and so, is called <strong>Full-Backup</strong>. Thus, in a computer program, we would have two cached arrays - one for \(v_k(s)\) and one for \(v_{k+1}(s)\).</p> <h2 id="policy-improvement">Policy Improvement</h2> <p>Once we have a policy, the next question is do we follow this policy or shift to a new improved policy? one way to answer this problem is to take any action that this policy does not suggest and then evaluate the same policy after that action. If the returns are higher then we can say that taking that action is better than following the current policy. The way we evaluate the action is through the action-value function:</p> \[q_\pi(s,a) = R^a_s + \gamma \sum_{s' \in S} P^{a}_{ss'} v_\pi(s')\] <p>If this value is greater than the value function of a state S, then that essentially means that it is better to select this action than follow the policy \(\pi\) , and by extension, it would mean that anytime we encounter state \(S\), we would like to take this action. So, let’s call the schema of taking action \(a\) every time we encounter \(S\) as a new policy \(\pi'\), and so, we can now say</p> \[q_\pi(s,\pi'(s)) \geq v_\pi(s)\] <p>This implies that the policy \(\pi'\) must be <strong>at-least</strong> as good as the policy \(\pi\):</p> \[v_{\pi'} \geq v_\pi\] <p>Thus, if we extend this idea to multiple possible actions at any state \(S\), the net incentive is to go full greedy on it and select the best out of all those possible actions:</p> \[\pi'(s) = \argmax_a q_\pi(s,a)\] <p>The greedy policy, thus, takes the action that looks best in the short term i.e after one step of lookahead. The point at which the new policy stops becoming better than the old one is the convergence point, and we can conclude that optimality has been reached. This idea also applies in the general case of stochastic policies, with the addition that in the case of multiple actions with the maximum value, a portion of the stochastic probability can be given to each.</p> <h2 id="policy-iteration">Policy Iteration</h2> <p>Following the greedy policy improvement process, we can obtain a sequence of policies:</p> \[\pi_0 \rightarrow v_{\pi_0} \rightarrow {\pi_1} \rightarrow v_{\pi_1} .... \rightarrow \pi_* \rightarrow v_{\pi_*}\] <p>Since a finite MDP has a finite number of policies, this process must converge at some point to an optimal value. This process is called <strong>Policy Iteration</strong>. The algorithm, thus, follows the process:</p> <ol> <li><strong>Evaluate</strong> the policy using the Bellman equation</li> <li><strong>Improve</strong> the policy using greedy policy improvement.</li> </ol> <p>A natural question that comes up at this point is that do we actually need to follow this optimization procedure all the way to the end? It does sound like a lot of work, and in a lot of cases, a workably optimal policy is actually reached much before the final iteration step, where the steps after achieving this policy add minimal improvement and thus, are somewhat redundant. Thus, we can include stopping conditions to tackle this, as follows:</p> <ol> <li>\(\epsilon\)-convergence</li> <li>Stop after \(k\) iteratiokns</li> <li>Value Iteration</li> </ol> <h2 id="value-iteration">Value Iteration</h2> <p>In this algorithm, the evaluation is truncated to one sweep → one backup of each state. To understand this, the first step is to understand something called the <strong>Principle of Optimality</strong>. The idea is that an optimal policy can be subdivided into two parts:</p> <ul> <li>An optimal first action \(A_*\)</li> <li>An optimal policy from the successor state \(S'\)</li> </ul> <p>So, if we know the solution to \(v_*(s')\) for all \(s'\) succeeding the state \(s\), then the solution can be found with just a one-step lookahead</p> \[v_*(s) \gets \max_{a \isin A} R^a_s + \gamma \sum_{\substack{s' \in S}} P^{a}_{ss'} v_{*} (s')\] <p>The intuition is to start from the final reward and work your way backward. There is no explicit update of policy, only values. This also opens up the possibility that the intermediate values might not correspond to any policy, and so interpreting anything midway will have some residue in addition to the greedy policy. In practice, we stop once the value function changes by only a small amount in a sweep. A summary of synchronous methods for DP is given by David Silverman:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| Problem      | Bellman Equation                   | Algorithm    |
|--------------|------------------------------------|--------------|
| Prediction   | Expectation Equation               | right 1      |
| Control      | Expectation Equation + Greedy Eval.| right 2      |
| Control      | Optimality Equation                | right 3      |
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[Dynamic programming (DP) is a method that solves a problem by breaking it down into sub-problems and then solving each sub-problem individually, after which it combining them into a solution. A good example is the standard Fibonacci sequence calculation problem, where traditionally the way to solve it would be through recursion]]></summary></entry><entry><title type="html">Bayesian Optimization</title><link href="https://amsks.github.io/blog/2023/bayesian-optimization/" rel="alternate" type="text/html" title="Bayesian Optimization"/><published>2023-07-08T16:57:00+00:00</published><updated>2023-07-08T16:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/bayesian-optimization</id><content type="html" xml:base="https://amsks.github.io/blog/2023/bayesian-optimization/"><![CDATA[<p>The general optimization problem can be stated as the task of finding the minimal point of some objective function by adhering to certain constraints. More formally, we can write it as</p> \[\min_x f(x) \,\,\,\, s.t \,\,\,\, g(x) \leq 0 \,\,\,, \,\,\, h(x) = 0\] <p>We usually assume that our functions \(f, g, h\) are differentiable, and depending on how we calculate the first and second-order gradients (The Jacobians and Hessians) of our function, we designate the different kinds of methods used to solve this problem. Thus, in a first-order optimization problem, we can evaluate our objective function as well as the Jacobian, while in a second-order problem we can even evaluate the Hessian. In other cases, we impose some other constraints on either the form of our objective function or do some tricks to approximate the gradients, like approximating the Hessians in Quasi-Newton optimization. However, these do not cover cases where \(f(x)\) is a black box. Since we cannot assume that we fully know this function our task can be re-formulated as finding this optimal point \(x\) while discovering this function \(f\). This can be written in the same form, just without the constraints</p> \[\min _x f(x)\] <h2 id="kwik">KWIK</h2> <p>To find the optimal \(x\) for an unknown \(f\) we need to explicitly reason about what we know about \(f\). This is the <strong>Knows What It Knows</strong> framework. I will present an example from the paper that helps understand the need for this explicit reasoning about our function. Consider the task of navigating the following graph:</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/AutoML/KWIK-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/AutoML/KWIK-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/AutoML/KWIK-1400.webp"/> <img src="/assets/img/AutoML/KWIK.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Each edge in the graph is associated with a binary cost and let’s assume that the agent does not know about these costs beforehand, but knows about the topology of the graph. Each time an agent moves from one node to another, it observes and accumulates the cost. An episode is going from the source on the left to the sink on the right. Hence, the learning task is to figure out the optimal path in a few episodes. The simplest solution for the agent is to assume that the costs of edges are uniform and thus, take the shortest path through the middle, which gives it a total cost of 13. We could then use a standard regression algorithm to fit a weight vector to this dataset and estimate the cost of the other paths, simply based on the nodes observed so far, which gives us 14 for the top, 13 for the middle, and 14 for the bottom paths. Hence, the agent would choose to take the middle path, even though it is suboptimal as compared to the top one.</p> <p>Now, let’s consider an agent that does not just fit a weight vector but reasons about whether it can obtain the cost of edges with the available data. Assuming the agent completed the first episode through the middle path and accumulated a reward of 13, the question it needs to answer is which path to go for next. In the bottom path cost of the penultimate node is 2, which can be figured out from the costs of nodes already visited</p> \[3 - 1 = 2\] <p>This gives us more certainty than the uniform assumption that we started with. However, this kind of dependence does not really exist for the upper node since the linear combination does not work on the nodes already visited. If we incorporate a way for our agent to say that it is not sure about the answer to the cost of the upper nodes, we can essentially incentivize it to explore the upper node in the next round, allowing our agent to visit this node and discover the optimal solution. This is similar to how we discuss the exploration-exploitation dilemma in Reinforcement Learning.</p> <h2 id="mdp-framework">MDP framework</h2> <p>Motivated from the previous section and based on the treatment done <a href="https://www.user.tu-berlin.de/mtoussai//teaching/Lecture-Maths.pdf">here</a>, we can model our solver as an agent and the function as the environment. Our agent can sample the value of the function in a range of possible values and in a limited budget of samples, it needs to find the optimal \(x\). The observation that comes after sampling from the environment is the noisy estimate of \(f\), which can call \(y\). Thus, we can write our function as the expectation over these outputs</p> \[f( x) = \mathbb{E}\big [ y |f(x) \big ]\] <p>We can cast this as a Markov Decision Process where the state is defined by the data the agent has collected so far. Let’s call this data \(S\). Thus, at each iteration \(t\), our agent exists in a state \(S_t\) and needs to make a decision on where to sample the next \(x_t\). Once it collects this sample, it adds this to its existing knowledge</p> \[S_{t+1} = S_t \cup \{x_t, f_t \}\] <p>We can create a policy \(\pi\) that our agent follows to take an action from a particular state</p> \[\pi : S_t \rightarrow x_t\] <table> <tbody> <tr> <td>Hence, the agent operates with a prior over our function \(P(f)\) , and based on this prior it calculates a deterministic posterior $$P_\pi (S</td> <td>x_t, f)$$ by multiplying it with the expectation over the outputs.</td> </tr> </tbody> </table> \[\pi ^* \in \text{argmin}_\pi \int P(f) P( S|\pi , f) \mathbb{E}[y|f]\] <p>Since the agent does not know \(f\) apriori, it needs to calculate a posterior belief over this function based on the accumulated data</p> \[P(f|S) = \frac{P(S|f) P(f)}{P(S)}\] <table> <tbody> <tr> <td>With the incorporation of this belief, we can define an MDP over the beliefs with stochastic transitions. The states in this MDP are the posterior belief $$P(f</td> <td>S)$$ . Thus, the agent needs to simulate the transitions in this MDP and it can theoretically solve the optimal problem through something like Dynamic programming. However, this is difficult to compute.</td> </tr> </tbody> </table> <h2 id="bayesian-methods">Bayesian Methods</h2> <table> <tbody> <tr> <td>This is where Bayesian methods come into the picture. They formulate this belief $$P(f</td> <td>S)$$ as a Bayesian representation and compute this using a gaussian process at every step. After this, they use a heuristic to choose the next decision. The Gaussian process used to compute this belief is called <strong>surrogate function</strong> and the heuristic used is called an <strong>Acquisition Function.</strong> We can write the process as follows:</td> </tr> </tbody> </table> <ol> <li>Compute the posterior belief using a surrogate Gaussian process to form an estimate of the mean \(\mu(x)\) and variance around this estimate \(\sigma^2(x)\) to describe the uncertainty</li> <li>Compute an acquisition function \(\alpha_t(x)\) that is proportional to how beneficial it is to sample the next point from the range of values</li> <li> <p>Find the maximal point of this acquisition function and sample at that next location</p> \[x_t = \argmax_x \alpha_t(x)\] </li> </ol> <p>This process is repeated a fixed number of iterations called the <strong>optimization budget</strong> to converge to a decently good point. Three poplar acquisition functions are</p> <ul> <li> <p><strong>Probability of Improvement (MPI) →</strong> The value of the acquisition function is proportional to the probability of improvement at each point. We can characterize this as the upper-tail CDF of the surrogate posterior</p> \[\alpha_t( x) = \int_{-\infty}^{y_{opt}}\mathcal{N} \big (y|\mu(x), \sigma (x) \big ) dy\] </li> <li> <p><strong>Expected Improvement (EI)</strong> → The value is not just proportional to the probability, but also to the magnitude of possible improvement from the point.</p> \[\alpha_t(x) = \int_{-\infty}^{y_{opt}}\mathcal{N} \big (y|\mu(x), \sigma (x) \big ) \big [ y_{opt} - y\big ] dy\] </li> <li> <p><strong>Upper Confidence Bound (UCB)</strong> → We control the exploration through the variance and control parameter and exploit the maximum values</p> \[\alpha_t(x) = -\mu(x) + \beta\sigma(x)\] </li> </ul> <p>The evaluation of this maximization of the acquisition function is another non-linear optimization problem. However, the advantage is that these functions are analytic and so, we can solve for jacobians and Hessians of these, ensuring convergence at least on a local level. To make this process converge globally, we need to optimize from multiple start points from the domain and hope that after all these random starts the maximum found by the algorithm is indeed the global one.</p> <h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2> <p>One of the places where Global Bayesian Optimization can show good results is the optimization of hyperparameters for Neural Networks. So, let’s implement this approach to tune the learning rate of an Image Classifier! I will use the KMNIST dataset and a small ResNet-9 Model with a Stochastic Gradient Descent optimizer. Our plan of attack is as follows:</p> <ol> <li>Create a training pipeline for our Neural Network with the Dataset and customizable learning rate</li> <li>Cast the training and inference into a an objective function, which can serve as ou blackbox</li> <li>Map the inference to an evaluation metric that can be used in the optimization procedure</li> <li>Use this function in a global bayesian optimization procedure.</li> </ol> <h3 id="creating-the-training-pipeline-and-objective-function">Creating the training pipeline and Objective Function</h3> <p>I have used PyTorch and the lightning module to create a boilerplate that can be used to train our network. Since KMNIST and ResNet architectures are already available in PyTorch, all we need to do is customize the ResNet architecture for MNIST, which I have done as follows</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_resnet9_model</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sh">'''</span><span class="s">
        Function to customize the RESNET to 9 layers and 10 classes

        Returns
        --------
        torch.module
            Pytorch Module of the Model
    </span><span class="sh">'''</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># 
</span><span class="k">class</span> <span class="nc">ResNet9</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
            Pytorch Lightning Module for training the RESNET with SGD optimizer

            Parameters
            -----------
            learning_rate: float 
                Learning rate to be used for training every time since it is an 
                optimization parameter
        </span><span class="sh">'''</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nf">create_resnet9_model</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

    <span class="nd">@auto_move_data</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_no</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="nf">self</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div> <p>Once this is done, our next step is to use the training pipeline and cast it into an objective function. For this, we need to evaluate our model somehow. I have used the balanced accuracy as an evaluation metric, but any other metric can also be used (like the AUC-ROC score)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span>  <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                <span class="n">gpu_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                <span class="n">iteration</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                <span class="n">model_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">./outputs/models/</span><span class="sh">'</span><span class="p">,</span> 
                <span class="n">train_dl</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">test_dl</span> <span class="o">=</span> <span class="bp">None</span> 
            <span class="p">):</span>

    <span class="sh">'''</span><span class="s">
        The objective function for the optimization procedure 

        Parameters
        -----------
        lr: float 
            learning Rate 
        epochs: int 
            Epochs for training
        gpu_count: int 
            Number of GPUs to be used (0 for only CPUs)
        iteration: int 
            Current iteration
        model_dir: str
            directory to save model checkpoints 
        train_dl: Torch Dataloader 
            Dataloader for training
        test_dl: Torch Dataloader 
            Dataloader for inference

        Returns
        ---------
        float
            balanced Accuracy of the model after inference

    </span><span class="sh">'''</span>

    <span class="n">save</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="sh">"</span><span class="s">current_model.pt</span><span class="sh">"</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet9</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="nc">Trainer</span><span class="p">(</span>
        <span class="n">gpus</span><span class="o">=</span><span class="n">gpu_count</span><span class="p">,</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">progress_bar_refresh_rate</span><span class="o">=</span><span class="mi">20</span>
    <span class="p">)</span>

    <span class="n">trainer</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">)</span>
    <span class="n">trainer</span><span class="p">.</span><span class="nf">save_checkpoint</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

    <span class="n">inference_model</span> <span class="o">=</span> <span class="n">ResNet9</span><span class="p">.</span><span class="nf">load_from_checkpoint</span><span class="p">(</span>
        <span class="n">checkpoint</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">true_y</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">prob_y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">test_dl</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">test_dl</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">true_y</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
				<span class="n">model</span><span class="p">.</span><span class="nf">freeze</span><span class="p">()</span>
		    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="nf">inference_model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		    <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pred_y</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">predicted_class</span><span class="p">.</span><span class="nf">cpu</span><span class="p">())</span>
        <span class="n">prob_y</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">probabilities</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">save</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
        <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nf">balanced_accuracy_score</span><span class="p">(</span><span class="n">true_y</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">))</span>
</code></pre></div></div> <h3 id="implementing-bayesian-optimization">Implementing Bayesian Optimization</h3> <p>As mentioned in the previous sections, we first need a Gaussian Process as a surrogate model. We can either write it from scratch or just use some open-sourced library to do this. Here, I have used sci-kit learn to create a regressor</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create the Model
</span>    <span class="n">m52</span> <span class="o">=</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">gaussian_process</span><span class="p">.</span><span class="nf">kernelsConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="nc">Matern</span><span class="p">(</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> 
                                        <span class="n">nu</span><span class="o">=</span><span class="mf">1.5</span>
                                    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">gaussian_process</span><span class="p">.</span><span class="nc">GaussianProcessRegressor</span><span class="p">(</span>
                                        <span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> 
                                        <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> 
                                        <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">100</span>
                                    <span class="p">)</span>
</code></pre></div></div> <p>Once th Gaussian process is established, we now need to write the acquisition function. I have used the Expected Improvements acquisition function. The core idea can be re-written as proposed by Mockus</p> \[EI(x) = \begin{cases} \big( \mu_t(x) - y_{max} - \epsilon \big ) \Phi(Z) + \sigma_t (x) \phi(Z) &amp;\sigma_t(x) &gt; 0 \\ 0 &amp; \sigma_t(x) &gt; 0 \end{cases}\] <p>Where</p> \[Z = \frac{\mu_t(x) - y_{max} - \epsilon}{\sigma_t(x) }\] <p>and \(\Phi\) and \(\phi\) are the PDF and CDF functions. This formulation is an analytical expression that achieves the same result as our earlier formulation and we have added \(\epsilon\) as an exploration parameter. This can be implemented as follows</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_acquisition</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
            Acquisition function using hte Expected Improvement method

            Parameters
            -----------
            X : N x 1 
                Array of parameter points observed so far

            X_samples : N x 1
                Array of Sampled points between the bounds

            Returns
            --------
            float
                Expected improvement

        </span><span class="sh">'''</span>

        <span class="c1"># calculate the max of surrogate values from history
</span>        <span class="n">mu_x_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">surrogate</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">max_x_</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">mu_x_</span><span class="p">)</span>

        <span class="c1"># Get the mean and deviation of the samples 
</span>        <span class="n">mu_sample_</span><span class="p">,</span> <span class="n">std_sample_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">surrogate</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">mu_sample_</span> <span class="o">=</span> <span class="n">mu_sample_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Get the improvement
</span>        <span class="k">with</span> <span class="n">np</span><span class="p">.</span><span class="nf">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="sh">'</span><span class="s">warn</span><span class="sh">'</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu_sample_</span> <span class="o">-</span> <span class="n">max_x_</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">/</span> <span class="n">std_sample_</span>
            <span class="n">EI_</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu_sample_</span> <span class="o">-</span> <span class="n">max_x_</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> \
                <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">std_sample_</span> <span class="o">*</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">EI_</span><span class="p">[</span><span class="n">std_sample_</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">EI_</span>
</code></pre></div></div> <p>the <code class="language-plaintext highlighter-rouge">self.surrogate()</code> function is just predicting using the Gaussian process earlier written. Once we have our expected improvements, we need to optimize our acquisition by maximizing over these expected improvements</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">optimize_acq</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
            Optimization of the Acquisition function using a maximization check of the outputs

            Parameters
            -----------
            X : N x 1 
                Array of parameter points

            Returns
            --------
            float
                Next location of the sampling point based on the Maximization

        </span><span class="sh">'''</span>
            
        <span class="c1"># Calculate Acquisition value for each sample
</span>        <span class="n">EI_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">X_samples_</span><span class="p">)</span>

        <span class="c1"># Get the index of the largest Score
</span>        <span class="n">max_index_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">EI_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">X_samples_</span><span class="p">[</span><span class="n">max_index_</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <h3 id="putting-it-all-together">Putting it all together</h3> <p>Now that we have our optimization routines, we just need to combine them with our objective function into a loop and we are done. In my code, I have implemented the optimization as a class and I pass the paramters to this class. So, the main loop looks as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">budget</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="nc">KMNIST</span><span class="p">(</span><span class="sh">"</span><span class="s">kmnist</span><span class="sh">"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="nc">KMNIST</span><span class="p">(</span><span class="sh">"</span><span class="s">kmnist</span><span class="sh">"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>

<span class="n">train_dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span> <span class="p">)</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># sample the domain
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">init_samples</span><span class="p">)])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">objective</span><span class="p">(</span><span class="n">lr</span> <span class="o">=</span><span class="n">x</span><span class="p">,</span> 
                        <span class="n">epochs</span><span class="o">=</span><span class="n">init_epochs</span><span class="p">,</span> 
                        <span class="n">gpu_count</span><span class="o">=</span><span class="n">gpu_count</span><span class="p">,</span>
                        <span class="n">train_dl</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span>
                        <span class="n">test_dl</span><span class="o">=</span><span class="n">test_dl</span>
                        <span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">budget</span><span class="p">):</span>
		<span class="c1"># fit the model
</span>		<span class="n">B</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
		
		<span class="c1"># Select the next point to sample
</span>		<span class="n">X_next</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">optimize_acq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
		
		<span class="c1"># Sample the point from Objective
</span>		<span class="n">Y_next</span> <span class="o">=</span> <span class="nf">objective</span><span class="p">(</span> <span class="n">lr</span><span class="o">=</span><span class="n">X_next</span><span class="p">,</span> 
		                    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
		                    <span class="n">gpu_count</span><span class="o">=</span><span class="n">gpu_count</span><span class="p">,</span>
		                    <span class="n">model_dir</span><span class="o">=</span> <span class="n">output_dir</span><span class="o">+</span><span class="sh">"</span><span class="s">/models/</span><span class="sh">"</span><span class="p">,</span> 
		                    <span class="n">iteration</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
		                    <span class="n">train_dl</span><span class="o">=</span> <span class="n">train_dl</span><span class="p">,</span>
		                    <span class="n">test_dl</span> <span class="o">=</span> <span class="n">test_dl</span>
		                    <span class="p">)</span>
		
		<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">LR = </span><span class="si">{</span><span class="n">X_next</span><span class="si">}</span><span class="s"> </span><span class="se">\t</span><span class="s"> Balanced Accuracy = </span><span class="si">{</span><span class="n">Y_next</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s"> %</span><span class="sh">"</span><span class="p">)</span>
		
		<span class="c1"># Plots for second iteration onwards 
</span>		<span class="n">B</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
		
		<span class="c1"># add the data to History
</span>		<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="n">X_next</span><span class="p">]]))</span>
		<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="p">[[</span><span class="n">Y_next</span><span class="p">]]))</span>
</code></pre></div></div> <p>Here, I have used a budged to 10 function evaluations in the main loop and 2 function evaluations before the first posterior estimate. An exemplary plot of what comes out at the end is shown below</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/AutoML/plot-iter-3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/AutoML/plot-iter-3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/AutoML/plot-iter-3-1400.webp"/> <img src="/assets/img/AutoML/plot-iter-3.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The vertical axis is the Balanced Accuracy and the horizontal axis is the learning rate. As can be seen, this is the third iteration of the main loop, with 2 points sampled as an initial estimate, and the acquisition function is the highest at the region with the balance of uncertainty and value of the mean.</p>]]></content><author><name></name></author><category term="automl"/><summary type="html"><![CDATA[The general optimization problem can be stated as the task of finding the minimal point of some objective function by adhering to certain constraints. More formally, we can write it as]]></summary></entry><entry><title type="html">Algebra of Genetic Algorithms</title><link href="https://amsks.github.io/blog/2023/algebra-of-genetic-algorithms/" rel="alternate" type="text/html" title="Algebra of Genetic Algorithms"/><published>2023-07-08T12:57:00+00:00</published><updated>2023-07-08T12:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/algebra-of-genetic-algorithms</id><content type="html" xml:base="https://amsks.github.io/blog/2023/algebra-of-genetic-algorithms/"><![CDATA[<h2 id="formalizing-genetic-algorithm-search">Formalizing genetic Algorithm Search</h2> <p>To formalize genetic algorithms we need to define inputs and outputs and formalize what this algorithm does through maps. So, let’s consider \(X\) to be the space of all solutions. From elements of this space of solutions, we can construct multisets by grouping them, so let’s take \(P_M(X)\) to be this maximal set of multi-sets. Thus, a population \(P\) can be thought of as one of these possible multisets and so</p> \[P \in P_M(X)\] <p>Now, the process of mutation and cross-over can be formalized as a genetic operator that maps \(P_M(X)\), which can also be thought of as mapping \(X^k\) to \(X\). Let \(\Omega\) be a genetic operator. Thus, we can say:</p> \[\Omega:P_M(X) \rightarrow P_M(X) \\ \Omega : X^k \rightarrow X \,\,\,\, \,\,\,\,\forall k \in \mathbb{N} \\ \implies \Omega (x_1, ..., x_k \in X) = x \in X\] <p>Thus, we can now classify the operators as follows:</p> <ul> <li>Recombination → \(\Omega : X^2 \rightarrow X\)</li> <li>Mutation → \(\Omega : X \rightarrow X\)</li> <li>Selection → \(\Omega: A \subseteq P_M(X) \rightarrow B \subseteq A\)</li> </ul> <h2 id="forma-analysis">Forma Analysis</h2> <h3 id="schema-theorem">Schema Theorem</h3> <p>Forma analysis creates a rigorous framework to look at how genetic algorithms might work on different problems and comes from Schema Theorem. According to the original paper, the Schema Theorem can be essentially summarized as</p> \[\mathbb{E} \{N_\xi(t+1) \} \geq N_\xi(t) \frac{\hat{\mu}_\xi(t)}{\bar{\mu}(t)} \bigg[ 1 - \sum_{\omega \in \Omega}p_\omega p_\omega ^\xi\bigg]\] <p>The various elements in this equation are:</p> <ul> <li>\(\xi\) → Schema which is like a template of strings in which certain positions are fixed. For example, a schema for binary strings like 1XX0X1 means that the first, fourth, and sixth positions have to be 1, 0, and 1, respectively, and the other positions can be either 0 or 1</li> <li>\(N_\xi(t)\) → Population size at time \(t\) that belongs to a schema \(\xi\)</li> <li>\(\hat{\mu}_\xi(t)\) → Average fitness of all the members of the population at time t that are instances (members) of the schema \(\xi\)</li> <li>\(\bar{\mu}(t)\) → average fitness of the whole population at time t</li> <li>\(\Omega\) → Set of genetic operators in Use</li> <li>\(p_\omega p_\omega ^\xi\) → This signified the potentially disruptive effect when we apply the operators \(\omega \in \Omega\) to members of schema \(\xi\)</li> </ul> <p>The theorem is saying that the expectation of the next generation under a schema is proportional to the relative fitness of the schema and inversely to the disruptive potential of the genetic operators on this schema. This disruptive potential is directly proportional to the length of the schema, in addition to the probability of other operators like mutation and crossover. Thus, short and low-order schemata with above-average fitness increase exponentially in frequency in successive generations. The ability of the schema theorem, which governs the behavior of a simple genetic algorithm, to lead the search to interesting areas of the space is governed by the quality of the information it collects about the space through observed schema fitness averages in the population → If the schemata tend to collect together solutions with related performance, then the fitness-variance of schemata will be relatively low, and the information that the schema theorem utilizes will have predictive power for previously untested instances of schemata that the algorithm may generate. On the other hand, if the performances are not related in the schemata then the solutions generated cannot be assumed to bear any relation to the fitness of the parents i.e they might just be random. Thus, we need to incorporate domain-specific knowledge in the representations that we use for our algorithm since that signified the underlying distribution that might relate to similar performances in the future. Now, it was proven in the 1990s that this schema theorem applied to any subset of the schema \(\xi\), and not just the whole schemata, under the constraint that we adjust the disruptive potential according to the subset. The generalized schema was termed Formae (Singular Forma) and this is how the theory around format came to be. Forma analysis allows us to develop genetic representations and operators that can maximize \(\hat{\mu_\xi(t)}\) by selecting subsets of \(\xi\) that are appropriate for the domain. This is done by constructing equivalence relations that partition the search space into appropriate equivalence classes which play the rôle of format.</p> <h3 id="equivalence-relations--basis">Equivalence Relations → Basis</h3> <p>The first step to forma analysis is to define relations (\(\sim\) ) on our search space \(X\). This is simply saying that each element of our search space can have a property that is either true or false. For example, we can define a greater than relation \(&gt; : X \rightarrow \{0,1\}\) that compares our element to some integer. Now, these relations are called equivalence relations if they are</p> <ul> <li>Reflexive → If each element of the domain is related to itself</li> <li>Symmetric → \(a \sim b \implies b \sim a\)</li> <li>Transitive → \(a \sim b , b \sim c \implies a \sim c\)</li> </ul> <p>Equivalence relations are essentially partitions of \(X\) since they partition it into equivalence classes. Going back to our example of a schema, if we are to consider our binary schema 1XX0X1 and generalize it to something like XXXXXX where X represents positions in a string that need to be specified and X represents the unspecified positions, then the equivalence relation here is that the 1, 4 and 6 positions need to be specified. Now, taking \(\{0,1\}\) as our alphabet one of the equivalence classes that are induced by this equivalence relation is 1XX0X1, but there can be others like 0XX0X1, or 1XX0X0. Thus, our equivalence relation induces multiple equivalence classes that then form the schema.</p> <p>Let us denote the set of all equivalence relations on \(X\) as \(E(X)\) . So, if we have an equivalence relation \(\psi \in E(X)\), then we can call \(E_\psi\) to be the set of equivalence classes induced by \(\psi\). This set of classes is called formae. Now, if we have a vector of relations, say \(\mathbf{\Psi} \in E(X)\) , then we call \(\Xi_\mathbf{\Psi}\) as the set of formae, given by:</p> \[\Xi_\mathbf{\Psi} := \prod_{i=1}^\mathbf{\Psi} \Xi_{\psi_i}\] <p>And we can also denote the union of the formae as</p> \[\Xi(\mathbf{\Psi}) := \bigcup_{\psi \in \mathbf{\Psi}} \Xi_\psi\] <p>Now, let’s consider a relation that lies at the intersection of all the members of \(\mathbf{\Psi}\) → \(\phi := \bigcap \mathbf{\Psi}\). This relation would induce equivalence classes that would be intersections of the classes induced by the elements of \(\mathbf{\Psi}\), and this result can be mathematically written as:</p> \[[x]_\phi = \bigcap \{ [x]_\psi \,\, |\,\, \psi \in \mathbf{\Psi} \}\] <p>We can also define the span of \(E(X)\) as a map from its power set onto itself</p> \[Span: \mathbb{P}(E(X)) \rightarrow \mathbb{P}(E(X))\] <table> <tbody> <tr> <td>If we have a condition where a set of relations \(B \in E(X)\) has members that cannot be constructed by intersecting any other members of \(B\), then \(B\) is called an independent set of relations. Also, \(B\) is said to be orthogonal to the order \(k\) if given any \(k\) equivalence classes induced by members of \(B\), their intersection is non-empty. If $$k =</td> <td>B</td> <td>\(, then we call\)B\(orthogonal. It has been shown that orthogonality implies independence, and so, we can use this concept to define a basis of\)\mathbf{\Psi}\(→ Any subset\)B\(of\)\mathbf{\Psi} \subseteq E(X)$$ will constitute a basis iff:</td> </tr> </tbody> </table> <ul> <li>\(B\) in independent</li> <li>\(B\) spans \(\mathbf{\Psi}\)</li> </ul> <p>Thus, if \(B\) is orthogonal then we have an orthogonal basis. Moreover, the number of elements in \(B\) determines the dimensions of our basis. This notion of orthogonality of the set is important as it helps us ensure that our mapping from representations to solutions is fully defined.</p> <h3 id="representations-through-basis">Representations through Basis</h3> <p>Once we have a basis, we can follow the vectorization procedure to vectorize \(\mathbf{\Psi}\) in terms of the elements of \(B\) → A general equivalence relation \(\mathbf{\Psi}\) can be decomposed into component basic equivalence relations in \(B\). Our first step would be to go from equivalence relations to representations, by defining a representation. We first define a partial representation function \(\rho\) for an equivalence relation \(b \in E(X)\):</p> \[\rho_{b} : X \rightarrow \Xi_b\] <p>Taking \([x]_b\) to be the equivalence class under the relation \(b,\) we can say</p> \[\rho_b(x) := [x]_b\] <p>Thus, if we have a set \(B = \{ b_1, b_2, ..., b_n\}\), we can define a genetic representation function as</p> \[\mathbf{\rho_B} := (\rho_{b_1}, ..., \rho_{b_2}) \,\,\,\,\, s.t \,\,\,\,\, \mathbf{\rho_B}: X \rightarrow \Xi_B \\ \implies \mathbf{\rho_B} (x) = ([x]_{b_1}, ..., [x]_{b_n})\] <p>Let \(C\) be the space of chromosomes (Representations), we can call this set the image of \(X\)under \(\mathbf{\rho_B}\) and if \(\mathbf{\rho}_B\) is injective, we can define a growth function \(g:C \rightarrow X\) as the inverse of the representation function:</p> \[g: \Xi_b \rightarrow X \\ g(\mathbf{\xi}) := \mathbf{\rho}_B^{-1}(\mathbf{\xi})\] <p>We now have a vector space over which we have created a way to map representations to Chromosomes and back, which allow us to define genetic operations through these functions.</p> <h3 id="coverage-and-unique-basis">Coverage and Unique Basis</h3> <p>Our next step is to understand how these equivalence relations can generate representations, and how the Chromosomes relate to these equivalence relations. To go towards usefulness, we first have to define something called Coverage → A set of equivalence relations \(\mathbf{\Psi} \subset E(X)\) is said to cover \(X\) if, for each pair of solutions in \(X\), there is at least one equivalence relation in \(\mathbf{\Psi}\) under which the solutions in the pair are not equivalent. Formally,</p> \[\forall x \in X, y \in X/\{x\} : \exists \psi \in \mathbf{\Psi} : \psi(x,y) = 0\] <p>The significance of this notion is easy to understand → Coverage is important because if a set of equivalence relations covers \(X\) then specifying to which equivalence class a solution belongs for each of the equivalence relations in the set suffices to identify a solution uniquely. By this definition, we can also prove that any basis \(B\) fo \(\mathbf{\Psi}\) would cover \(X\) if it covers \(\mathbf{\Psi}\) and extend it further to show that any orthogonal basis of \(X\) that also covers it can be a faithful representation of \(X\). This is the point that we have been trying to dig-into through formalism → The information that this orthogonal basis includes in its formulation is critical to the impact of genetic algorithm in search.</p> <h3 id="genes-and-alleles">Genes and Alleles</h3> <p>We can define the Genes as the members of the basis \(B\) of \(\mathbf{\Psi}\) and the members of \(\Xi_B\) will be called the formae, or the alleles.</p> <p>Using our basis we can track the information it transmits by checking the equivalence of the solutions generated under the relations in \(B\). This is called the Dynastic Potential → Given a basis \(B\) for a set of relations \(\mathbf{\Psi} \subset E(X)\) that covers \(X\), the dynastic potential \(\Gamma\) of a subset \(L \subseteq X\) is the set of all solutions in \(X\) that are equivalent to at least one member of \(L\) under the equivalence relations in \(B\).</p> \[\Gamma: P(X) \rightarrow P(X) \\ \Gamma(L) := \big \{ x \in X | \,\,\, \forall b \in B : \exists l \subset L: b(l,x) = 1 \big \}\] <p>Thus, the dynastic potential of \(L\) would be the set of all children that can be generated using only alleles available from the parent solutions in L. The solutions in \(L\) belong to different equivalence classes or formae. Thus, by measuring how many formae include solutions in \(L\). This is called the similarity set, formally defined as the intersection of all the formae to which solutions in \(L\) can belong:</p> \[\Sigma(L) := \begin{cases} \bigcap \{ \xi \in \Xi \,\, | \,\, L \subset \xi \}, \,\,\,\, if\,\, \exists \xi \in \Xi: L \subset \xi \\ X, \,\,\,\, otherwise \end{cases}\] <p>Now, it has been proved that the dynastic potential is contained by the similarity set</p> \[\forall L\subset X: \Gamma(L) \subset \Sigma(L)\] <p>Thus, we now have a full mathematical mechanism to se how the optimization process evolves:</p> <ol> <li>We have a representation of genes as our Basis of equivalence relations</li> <li>These genes map to alleles through a vector of partial representations \(\mathbf{\rho}_B\)</li> <li>The chromosomes then evolve to give a new set of genes through the growth function \(g\) after applying genetic operators \(\Omega\) to the representations</li> <li>The information that survives this process is quantified by the dynastic potential \(\Gamma\) of the solution space hence generated.</li> </ol>]]></content><author><name></name></author><category term="evolutionary-computation"/><summary type="html"><![CDATA[Formalizing genetic Algorithm Search]]></summary></entry><entry><title type="html">Non-Parametric Methods for Meta-Learning</title><link href="https://amsks.github.io/blog/2023/non-parameteric-approaches/" rel="alternate" type="text/html" title="Non-Parametric Methods for Meta-Learning"/><published>2023-07-07T16:57:00+00:00</published><updated>2023-07-07T16:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/non-parameteric-approaches</id><content type="html" xml:base="https://amsks.github.io/blog/2023/non-parameteric-approaches/"><![CDATA[<p>The optimization-based methods are very useful for model-agnosticism and expression with sufficiently deep networks. However, as we have seen the main bottleneck is the second-order optimization which ends up being compute and memory intensive. Thus, the natural question is whether we can embed a learning procedure without the second-order optimization? one answer to this lies in the regime of data when it comes to the test time → during the meta-test time our paradigm of few-shot learning is a low data regime. Thus, methods that are non-parametric and have been shown to work well in these cases can be applied here! Specifically,</p> <ul> <li>We want to be parametric during hte trainng phase</li> <li>We can apply a non-parametric way to compare classes during test time</li> </ul> <p>Thus, the question now becomes → Can we use parametric Learners to produce effective non-parametric learners? The straight answer to this would be something like K-Nearest Neighbors where we take a test sample and compare it against our training classes to see which one I the closest. However, now we have the issue of the notion of closeness! In the supervised case, we could simply use a L2-Norm, but this might not be the case for meta-learning strategies since direct comparison of low-level features might fail to take into account meta-information that might be relevant. Hence, we look to other approaches</p> <h3 id="siamese-networks">Siamese Networks</h3> <p>A siamese network is an architecture which was multiple sub-networks with identical configuration i.e same weights and hyperparameters. We train this architecture to output the similarity between two inputs using the feature vectors.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Meta/Meta-1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Meta/Meta-1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Meta/Meta-1-1400.webp"/> <img src="/assets/img/Meta/Meta-1.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>In our case, we can train these networks to predict whether or not two images belong to the same class or not and thus, we have a black box similarity between our test data and our training classes. We can use this during the test to compare the input with all classes in our training set and output the class with the highest similarity.</p> <h3 id="matching-networks">Matching Networks</h3> <p>The issue with the siamese network approach is that we are training the network on Binary classification but testing it on Multi-Class classification. <a href="https://arxiv.org/pdf/1606.04080.pdf">Matching Networks</a> circumvent this problem by training the networks in such a way that the Nearest-Neighbors method produces good results in the learned embedding space.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Meta/Meta-2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Meta/Meta-2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Meta/Meta-2-1400.webp"/> <img src="/assets/img/Meta/Meta-2.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>To do this, we learn an embedding space \(g(\theta)\) for all input classes and also create an embedding space \(h(\theta)\) for the test data. We then compare the \(g(.)\) and \(f(.)\) to predict each image class, which is then summed to create a test prediction. Each of the black dots in the above image corresponds to the comparison between training and test images, and our final prediction is the weighted sum of these individual labels</p> \[\hat{y}^{ts} = \sum _{x_k, y_k \in \mathcal{D^{tr}}} f _\theta(x^{ts}, x_k) y_k\] <p>This paper used a bi-directional LSTM to produce \(g_\theta\) and a convolutional encoder to embed the images in \(h_\theta\) and the model was trained end-end, with the training and test doing the same thing. The general algorithm for this is as follows:</p> <ol> <li>Sample Task \(\mathcal{T}_i\) ( a sequential stream or mini-batches )</li> <li>Sample Disjoint Datasets \(\mathcal{D^{tr}_i}\),\(\mathcal{D^{test}_i}\) from \(\mathcal{D}_i\)</li> <li>Compute \(\hat{y}^{ts} = \sum _{x_k, y_k \in \mathcal{D^{tr}}} f _\theta(x^{ts}, x_k) y_k\)</li> <li>Update \(\theta\) using \(\nabla_\theta\mathcal{L}(\hat{y}^{ts}, y^{ts})\)</li> </ol> <h3 id="prototypical-networks">Prototypical Networks</h3> <p>While Matching networks work well for one-shot meta-learning tasks, they do all this for only one class. If we had a problem of more than one shot prediction, then the matching network will do the same process for each class, and this might not be the most efficient method for this. <a href="https://arxiv.org/abs/1703.05175">Prototypical Networks</a> alleviate this issue by aggregating class information to create a prototypical embedding → If we assume that for each class there exists an embedding in which points cluster around a single prototype representation, then we can train a classifier on this prototypical embedding space. This is achieved through learning a non-linear map from the input into an embedding space using a neural network and then taking a class’s prototype to be the mean of its support set in the embedding space.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Meta/Meta-3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Meta/Meta-3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Meta/Meta-3-1400.webp"/> <img src="/assets/img/Meta/Meta-3.png" class="img-centered rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>As shown in the figure above, the classes become seperable in this embedding space.</p> \[\mathbf{c}_k = \frac{1}{|\mathcal{D_i^{tr}}|} \sum_{(x,y) \in \mathcal{D_i^{tr}}} f_\theta(x)\] <p>Now, if we have a metric on this space then all we have to do is find the near class cluster to a new query point and we can classify that point as belonging to this class by taking the softmax over the distances</p> \[p_\theta(y = k|x) = \frac{\exp \big( -d (f_\theta(x), \mathbf{c}_k) \big)}{ \sum_{k'} \exp \big( -d (f_\theta(x), \mathbf{c}_{k'}) \big)}\] <p>If we want to reason more complex stuff about our data then we just need to create a good enough representation in the embedding space. Some approaches to do this are:</p> <ul> <li><a href="https://arxiv.org/abs/1711.06025">Relation Network</a> → This is an approach where they learn the relationship between embeddings i.e instead of taking \(d(.)\) as a pre-determined distance measure, they learn it inherently for the data</li> <li><a href="https://arxiv.org/pdf/1902.04552.pdf">Infinite Mixture Prototypes</a> → Instead of defining each class by a single cluster. they represent each class by a set of clusters. Thus, by inferring this number of clusters they are able to interpolate between nearest neighbors and the prototypical representations.</li> <li><a href="https://arxiv.org/abs/1711.04043">Graph Neural Networks</a> → They extend the Matching network perspective to view the few-shot problem as the propagation of label information from labeled samples towards the unlabeled query image, which is then formalized as a posterior inference over a graphical model.</li> </ul> <h2 id="comparing-meta-learning-methods">Comparing Meta-Learning Methods</h2> <h3 id="computation-graph-perspective">Computation Graph Perspective</h3> <p>We can view these meta-learnin algorithms as computation graphs:</p> <ul> <li>Black-box adaptation is essentially a sequence of inputs and outputs on a computational graph</li> <li>Optimization can be seen as a embedding an optimization routine into a computational graph</li> <li>Non-parameteric methods can be seen as computational graphs working with the embedding spaces</li> </ul> <p>This viewpoint allows us to see how to mix-match these approaches to improve performance:</p> <ul> <li><a href="https://openreview.net/forum?id=BJfOXnActQ">CAML</a> → Ths approach creates an embedding space, which is also a metric space, to capture inter-class dependencies. once this is done, they run gradient descent using this metric</li> <li><a href="https://arxiv.org/abs/1807.05960">Latent Embedding Optimization (LEO)</a> → They learn a data-dependent latent generative representation of model parameters and then perform gradient-based meta-learning on this space. Thus, this approach decouples the need for GD on higher dimensional space by exploiting the topology of the data.</li> </ul> <h3 id="algorithmic-properties-perspective">Algorithmic Properties Perspective</h3> <p>We consider the following properties of the most importance for most tasks :</p> <ul> <li><strong>Expressive Power</strong> → Ability of our learned function \(f\) to represent a range of learning procedures. This is important for scalability and applicability to a range of domains.</li> <li><strong>Consistency</strong> → The learned learning procedure will asymptotically solve the task given enough data, regardless of the meta-training procedure. The main idea is to reduce the reliance on the meta-training and generate the ability to perform on Out-Of-Distribution (OOD) tasks</li> <li><strong>Uncertainty Awareness</strong> → Ability to reason about ambiguity during hte learning process. This is especially important for things like active learning, calibrated uncertainty, and RL.</li> </ul> <p>We can now say the following about the three approaches:</p> <ul> <li><strong>Black-Box Adaptation</strong> → Complete Expressive Power, but not consistent</li> <li><strong>Optimization Approaches</strong> → Consistent and expressive for sufficiently deep models, but fail in expressiveness for other kinds of tasks, especially in Meta-Reinforcement Learning.</li> <li><strong>Non-parametric Approaches</strong> → Expressive for most architectures and consistent under certain conditions</li> </ul>]]></content><author><name></name></author><category term="meta-learning"/><summary type="html"><![CDATA[The optimization-based methods are very useful for model-agnosticism and expression with sufficiently deep networks. However, as we have seen the main bottleneck is the second-order optimization which ends up being compute and memory intensive. Thus, the natural question is whether we can embed a learning procedure without the second-order optimization? one answer to this lies in the regime of data when it comes to the test time → during the meta-test time our paradigm of few-shot learning is a low data regime. Thus, methods that are non-parametric and have been shown to work well in these cases can be applied here! Specifically,]]></summary></entry><entry><title type="html">Parametric Methods for Meta-Learning</title><link href="https://amsks.github.io/blog/2023/parametric-methods-for-meta-learning/" rel="alternate" type="text/html" title="Parametric Methods for Meta-Learning"/><published>2023-07-07T15:57:00+00:00</published><updated>2023-07-07T15:57:00+00:00</updated><id>https://amsks.github.io/blog/2023/parametric-methods-for-meta-learning</id><content type="html" xml:base="https://amsks.github.io/blog/2023/parametric-methods-for-meta-learning/"><![CDATA[<h2 id="back-box-adaptation">Back-Box Adaptation</h2> <p>These are a set of approaches that treat step 1 as an inference problem and thus, training a Neural Network to represent \(p(\phi_i \mid \mathcal{D}^{tr}, \theta)\) i.e a way to estimate \(\phi_i\) and then use that as a parameter to optimize for a new task. The deterministic way to go about it would be to take point estimates</p> \[\phi_i = f_\theta (\mathcal{D}^{tr}_i)\] <p>Thus, we can treat \(f_\theta(.)\) as a neural network parameterized by \(\theta\) which takes the training data as an input, sequential or batched, and outputs the task-specific parameters \(\phi_i\) which are then used by another neural network \(g_{\phi_i} (.)\) to predict the outputs on a new dataset. Thus, we can essentially treat this as a supervised learning problem with our optimization being</p> \[\begin{aligned} &amp; \max_\theta \sum_{\mathcal{T_i}} \sum_{(x,y) \sim \mathcal{D_i}^{test}} \log g_{\phi_i} (y\mid x) \\ = &amp; \max_\theta \sum_{\mathcal{T_i}} \mathcal{L}(f_\theta(\mathcal{D^{tr}_i}), \mathcal{D_i^{test}}) \end{aligned}\] <p>To make this more tractable, \(\phi\) can be replaced by a sufficient statistic \(h_i\) instead of all the parameters. Some ANN architectures that work well with this approach are LSTMs, as shown in the work of <a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et. al</a>, feedforward networks with averaging as shown by <a href="https://arxiv.org/abs/1807.01613">Ramalho et. al</a>, Having inner task learners and outer meta-learners i.e <a href="https://arxiv.org/abs/1703.00837">Meta-Networks byMukhdalai</a> e.t.c. I am personally fascinated by the use of transformer architectures in this domain. The advantage of this approach is that it is expressive and easy to combine with other techniques like supervised learning, reinforcement learning e.t.c. However, the optimization bit is challenging and not the best solution from the onset for every kind of problem. Thus, our step-by-step approach would be:</p> <ol> <li>Sample Task \(\mathcal{T}_i\) ( a sequential stream or mini-batches )</li> <li>Sample Disjoint Datasets \(\mathcal{D^{tr}_i}\),\(\mathcal{D^{test}_i}\) from \(\mathcal{D}_i\)</li> <li>Compute \(\phi_i \leftarrow f_\theta(\mathcal{D^{tr}_i})\)</li> <li>Update \(\theta\) using \(\nabla_\theta\mathcal{L}(\phi_i, \mathcal{D^{test}_i})\)</li> </ol> <h2 id="optimization-based-approaches">Optimization-Based Approaches</h2> <p>This set treats the prediction of \(\phi_i\) as an optimization procedure and then differentiates through that optimization process to get a \(\phi_i\) that leads to good performance. The method can be summarized into the surrogates sums of maximization of observing the training data given \(\phi_i\) and the maximization of getting \(\phi_i\) given our model parameters \(\theta\).</p> \[\max_{\phi_i} \log p(\mathcal{D^{tr}_i} \mid \phi_i ) + \log p(\phi_i \mid \theta)\] <p>The second part of the above summation is our prior and the first part is a likelihood. Thus, our next question is the form of this prior that might be useful. In deep learning, one good way to incorporate priors is through the initialization of hyperparameters, or fine-tuning. Thus, we can take \(\theta\) as a pre-trained parameter and run gradient descent on it</p> \[\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L} (\theta, \mathcal{D^{tr}})\] <p>One popular way to do this for image classification is to have a feature extractor pre-trained on some datasets like ImageNet and then fine-tune its output to our problem. The aim in optimization-based approaches is to get to a sweet-spot in the multidimensional parameter space \(\mathbf{\Phi} = {\phi_1, \phi_2, .., \phi_n}\) such that our model becomes independent of the loss function and the training data, and this is called Model-Agnostic Meta-Learning. Thus, now our procedure becomes</p> <ol> <li>Sample Task \(\mathcal{T}_i\) ( a sequential stream or mini-batches )</li> <li>Sample Disjoint Datasets \(\mathcal{D^{tr}_i}\),\(\mathcal{D^{test}_i}\) from \(\mathcal{D}_i\)</li> <li>Optimize \(\phi_i \leftarrow f_\theta(\mathcal{D^{tr}_i})\)</li> <li>Update \(\theta\) using \(\nabla_\theta\mathcal{L}(\phi_i, \mathcal{D^{test}_i})\)</li> </ol> <p>For our optimization process, let’s define our final task specific parameter as</p> \[\phi = u(\theta, \mathcal{D^{tr}})\] <p>And now, our optimization target becomes</p> \[\begin{aligned} &amp; \min_\theta \mathcal{L}(\phi, \mathcal{D^{test}}) \\ = &amp; \min_\theta \mathcal{L} \big (u(\theta, \mathcal{D^{tr}}), \mathcal{D^{test}} \big) \end{aligned}\] <p>This optimization can be achieved by differentiating our loss w.r.t our meta-parameters \(\theta\) and then performing an inner differentiation w.r.t \(\phi\):</p> \[\frac{d\mathcal{L} (\phi, \mathcal{D^{test}} ) }{d \theta} = \nabla _{\bar{\phi}} \mathcal{L} (\bar{\phi}, \mathcal{D^{test}} ) \mid_{\bar{\phi} = u(\theta, \mathcal{D^{tr}}) } d_\theta \big ( u(\theta, \mathcal{D^{tr}} ) \big )\] <p>Now, if we use our optimization update for \(u (.)\) then we get:</p> \[\begin{aligned} &amp; u(\theta, \mathcal{D^{tr}} ) = \theta - \alpha \,\, d_\theta \big( L(\theta, \mathcal{D^{tr}}) \big ) \\ \implies &amp; d_\theta \big ( u(\theta, \mathcal{D^{tr}} ) \big ) = \mathbf{1} - \alpha \, d^2_\theta \big (L(\theta, \mathcal{D^{tr}}) \big ) \end{aligned}\] <p>Thus, when we substitute the hessian in the derivative equation we get:</p> \[\begin{aligned} \frac{d\mathcal{L} (\phi, \mathcal{D^{test}} ) }{d \theta} &amp; = \bigg (\nabla _{\bar{\phi}} \mathcal{L} (\bar{\phi}, \mathcal{D^{test}} ) \mid_{\bar{\phi} = u(\theta, \mathcal{D^{tr}}) } \bigg ). \bigg ( \mathbf{1} - \alpha \, d^2_\theta \big (L(\theta, \mathcal{D^{tr}}) \big ) \bigg ) \\ &amp; = \nabla _{\bar{\phi}} \mathcal{L} (\bar{\phi}, \mathcal{D^{test}} ) \mid_{\bar{\phi} = u(\theta, \mathcal{D^{tr}}) } - \alpha\,\, \bigg( \nabla _{\bar{\phi}} \mathcal{L} (\bar{\phi}, \mathcal{D^{test}} ) . d^2_\theta \big (L(\theta, \mathcal{D^{tr}}) \big ) \bigg ) \mid_{\bar{\phi} = u(\theta, \mathcal{D^{tr}}) } \end{aligned}\] <p>We now have a matrix product on the right which can be made more efficient and turn out ot be easier to compute than the full hessian of the network. Thus, this process is tractable. one really interesting thing that comes out of this is that we can also view this model-agnostic approach and the optimization update as a computation graph! Thus, we can say</p> \[\phi_i = \theta - f(\theta, \mathcal{D_i^{tr}}, \nabla_\theta \mathcal{L} )\] <p>Now, we can train an ANN to output the gradient \(f(.)\) , and thus, this allows us to mix the optimization procedure with the black-box adaptation process. Moreover, MAML approaches show a better performance on the omniglot dataset since they are optimizing for the model-agnostic points. It has been shown by <a href="https://arxiv.org/abs/1710.11622">Finn and Levine</a> that MAML can approximate any function of \(\mathcal{D_i^{tr}}\) and \(x^{ts}\) give:</p> <ul> <li>Non-zero \(\alpha\)</li> <li>Loss function gradient does not lose information about the label</li> <li>Data-points in \(\mathcal{D_i^{tr}}\) are unique</li> </ul> <p>Thus, MAML is able to inject inductive bias without losing expressivity.</p> <h3 id="inferece">Inferece</h3> <p>To better understand why MAML works well, we need to look through probabilistic lenses again to say that the meta-parameters \(\theta\) are inducing some kinds of prior knowledge into our system and so our learning objective would be to maximize the probability of observing the data \(\mathcal{D}_i\), given our meta-parameters \(\theta\)</p> \[\max_\theta \log \prod_i p(\mathcal{D}_i| \theta )\] <p>This can be further written as the sum of the probabilities of \(\mathcal{D_i}\) given our model-specific parameters \(\phi_i\), and the probability of seeing each \(\phi_i\) given our prior knowledge \(\theta\) :</p> \[\max _\theta \prod_i \int p(\mathcal{D_i} |\phi_i) p(\phi_i|\theta) d\phi_i\] <p>And now, we can estimate the probability of seeing each \(\phi_i\) given our prior knowledge \(\theta\) using a Maximum A-Posteriori (MAP) estimate \(\hat{\phi}\), so that</p> \[\max_\theta \log \prod_i p(\mathcal{D}_i| \theta ) \approx \max_\theta \log \prod_i p(\mathcal{D}_i|\hat{\phi}_i) p(\hat{\phi} | \theta)\] <p><a href="https://regijs.github.io/papers/laa96.pdf">It has been shown</a> that, for likelihoods that are Gaussian in \(\phi_i\), gradient descent with early stopping corresponds exactly to maximum a-posteriori inference under a Gaussian prior with mean initial samples. This estimation is exact in the linear case, and the variance in non-linear cases is determined by the order of the derivative. Thus, by limiting the computation to second derivatives, MAML is able to maintain a fairly good MAP inference estimate and so, MAML approximates hierarchical Bayesian Inference. We can also use other kinds of priors like:</p> <ul> <li> <table> <tbody> <tr> <td><a href="https://arxiv.org/abs/1909.04630">Explicit Gaussian Prior</a>: $$\phi \leftarrow \min_{\phi’} \mathcal{L} (\phi’, \mathcal{D^{tr}}) + \frac{\lambda}{2}</td> <td> </td> <td>\theta - \phi’</td> <td> </td> <td>^2$$</td> </tr> </tbody> </table> </li> <li><a href="https://arxiv.org/abs/1807.08912">Bayesian Linear Regression</a> on learned features</li> <li>Convex optimization on learned features</li> <li>Ridge or logistic regression</li> <li>Support Vector Machines</li> </ul> <h3 id="challenge-1-choosing-architecture">Challenge 1: Choosing Architecture</h3> <p>The major bottleneck in this process is the inner gradient step and so, we want to chosse an architecture that is effective for this inner step. One idea, called <a href="https://arxiv.org/abs/1806.06927">Auto-Meta</a> is to adopt the progressive neural architecture search to find optimal architectures for meta-learners i.e combine AutoML with Gradient-Based Meta-Learning. The interesting results of this were:</p> <ul> <li>They found highlynon-standard architectures, both deep and narrow</li> <li>They found architectures very different from the ones used for supervised learning</li> </ul> <h3 id="challenge-2-handling-instabilities">Challenge 2: Handling Instabilities</h3> <p>Another challenge comes from the instability that can come from the complicated Bi-Level optimization procedure. One way of mitigating this is to learn the inner vector learning rate and then tune the outer learning rate :</p> <ul> <li><a href="https://arxiv.org/abs/1707.09835">Meta-Stochastic Gradient Descent</a> is a meta-learner that can learn initialization, learner update direction, and learning rate, all in a single closed-loop process</li> <li><a href="https://arxiv.org/abs/1905.07435">AlphaMAML</a> incorporates an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates</li> </ul> <p>Another idea idea is to optimize only a subset of parameters in the innter loop:</p> <ul> <li><a href="https://arxiv.org/abs/1802.03596">DEML</a> jointly learns a concept generator, a meta-learner, and a concept discriminator. The concept generator abstracts representation of each instance of training data, the meta-learner performs few-shot learning on this instance and the concept discriminator recognizes the concept pertaining to each instance</li> <li><a href="https://arxiv.org/abs/1810.03642">CAVIA</a> partitions the model parameters into context parameters that serve as additional input to the model and are adapted on individual tasks and shared parameters that are meta-trained and shared across tasks. Thus, during test time only the context parameters need to be updated, which is a lower-dimensional search problem compared to all the model parameters</li> </ul> <p>In <a href="https://arxiv.org/pdf/1810.09502.pdf">MAML++</a> the authors ablate the various ideas and issues of MAML and then propose a new framework that addresses these issues. Some significant points were the de-coupling of the inner loop learning rate and the outer updates, the addition of batch normalization to each and every step and greedy updates.</p> <h3 id="challenge-3-managing-compute-and-memory">Challenge 3: Managing Compute and Memory</h3> <p>The backpropagation through many inner-gradient steps adds computational and memory overhead that is hard to deal with. One idea to mitigate this is to approximate the derivative of \(\phi_i\) w.r.t \(\theta\). This is a crude approximation and works well for few-shot learning problem, but fails in more complex problems like imitation learning. Another direction is to try to not compute the gradient at all and use the <a href="https://arxiv.org/abs/1909.04630">implicit function theorem</a>→ Let’s take our function \(\phi\) as the explicit gaussian representation :</p> \[\phi = u(\theta, \mathcal{D^{tr}}) = \underset{\phi'}{\text{argmin}} \mathcal{L}(\phi', \mathcal{D^{tr}}) + \frac{\lambda}{2} ||\phi' - \theta ||^2\] <p>Let our optimization function be</p> \[G(\phi', \theta ) = \mathcal{L}(\phi', \mathcal{D^{tr}}) + \frac{\lambda}{2} ||\phi' - \theta ||^2\] <p>Finding the \(\text{argmin}\) of the this function implies that the gradient w.r.t \(\phi\) is \(0\) i.e</p> \[\begin{aligned} &amp; \nabla_{\phi'} G(\phi', \theta) \big|_{\phi' = \phi} = 0 \\ \implies &amp; \nabla_\phi L(\phi) + \lambda(\phi - \theta ) = 0 \\ \implies &amp; \phi = \theta - \frac{1}{\lambda} \nabla_\phi L(\phi) \end{aligned}\] <p>Thus, our derivative now becomes</p> \[\begin{aligned} &amp; \frac{d \phi}{d \theta } = \mathbf{1} - \frac{1}{\lambda} \nabla_\phi^2 L(\phi) \frac{d \phi}{d \theta } \\ \therefore\,\,\,&amp; \frac{d \phi}{d \theta } = \bigg [\mathbf{1} + \frac{1}{\lambda} \nabla_\phi^2 L(\phi) \bigg ] ^{-1} \end{aligned}\] <p>Thus, we can compute this without going through the inner optimization process and it works only on the assumption that the out function \(G(\phi', \theta)\) has an \(\text{argmin}\) , to begin with.</p>]]></content><author><name></name></author><category term="meta-learning"/><summary type="html"><![CDATA[Back-Box Adaptation]]></summary></entry></feed>